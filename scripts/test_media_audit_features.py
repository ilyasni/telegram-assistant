#!/usr/bin/env python3
"""
Context7: –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ç–µ—Å—Ç –Ω–æ–≤–æ–≤–≤–µ–¥–µ–Ω–∏–π –∞—É–¥–∏—Ç–∞ –º–µ–¥–∏–∞ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

–ü—Ä–æ–≤–µ—Ä—è–µ—Ç:
1. –û–±—Ä–∞–±–æ—Ç–∫—É –º–µ–¥–∏–∞-–∞–ª—å–±–æ–º–æ–≤
2. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é Vision –≤ tagging (–ø–æ—Å—Ç—ã —Å –∫–æ—Ä–æ—Ç–∫–∏–º —Ç–µ–∫—Å—Ç–æ–º + –º–µ–¥–∏–∞)
3. –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –º–µ–¥–∏–∞
4. Media_sha256_list –≤ —Å–æ–±—ã—Ç–∏—è—Ö posts.parsed
5. –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω: Parse ‚Üí Media ‚Üí Vision ‚Üí Tagging ‚Üí Enrichment

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python scripts/test_media_audit_features.py --check-real-data
    python scripts/test_media_audit_features.py --test-post-id <uuid>
    python scripts/test_media_audit_features.py --full-pipeline-test
"""

import asyncio
import os
import sys
import argparse
import json
from datetime import datetime, timezone
from typing import Optional, Dict, Any, List
from uuid import UUID

# Context7: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

try:
    from worker.events.schemas.posts_parsed_v1 import PostParsedEventV1
    from worker.events.schemas.posts_vision_v1 import VisionUploadedEventV1, MediaFile
except ImportError:
    from events.schemas.posts_parsed_v1 import PostParsedEventV1
    from events.schemas.posts_vision_v1 import VisionUploadedEventV1, MediaFile

import asyncpg
import redis.asyncio as redis
from sqlalchemy import create_engine, text


def get_db_connection_string() -> str:
    """Context7: –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î (–∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–¥–∏–Ω—É—é —É—Ç–∏–ª–∏—Ç—É)."""
    from shared.utils.db_connection import get_database_url
    return get_database_url(kind="rw", async_=False)


async def check_real_data_status() -> Dict[str, Any]:
    """
    Context7: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–æ–≤–æ–≤–≤–µ–¥–µ–Ω–∏–π.
    
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç:
    - –ü–æ—Å—Ç—ã —Å –∞–ª—å–±–æ–º–∞–º–∏ (grouped_id –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–µ–¥–∏–∞)
    - –ü–æ—Å—Ç—ã —Å –∫–æ—Ä–æ—Ç–∫–∏–º —Ç–µ–∫—Å—Ç–æ–º + –º–µ–¥–∏–∞ (–¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–æ–≤–æ–π –ª–æ–≥–∏–∫–∏ tagging)
    - –ü–æ—Å—Ç—ã —Å Vision —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    - –ü–æ—Å—Ç—ã —Å media_sha256_list –≤ —Å–æ–±—ã—Ç–∏—è—Ö
    """
    print("=" * 80)
    print("üìä –ü–†–û–í–ï–†–ö–ê –†–ï–ê–õ–¨–ù–´–• –î–ê–ù–ù–´–• –î–õ–Ø –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø –ù–û–í–û–í–í–ï–î–ï–ù–ò–ô")
    print("=" * 80)
    
    conn = await asyncpg.connect(get_db_connection_string())
    
    try:
        results = {}
        
        # 1. –ü–æ—Å—Ç—ã —Å –º–µ–¥–∏–∞ (–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –∞–ª—å–±–æ–º—ã)
        posts_with_media = await conn.fetch("""
            SELECT 
                p.id,
                p.channel_id,
                p.telegram_message_id,
                p.content,
                p.has_media,
                p.created_at,
                (SELECT COUNT(*) FROM post_media_map pmm WHERE pmm.post_id = p.id) as media_count,
                (SELECT COUNT(*) FROM post_media pm WHERE pm.post_id = p.id) as legacy_media_count
            FROM posts p
            WHERE p.has_media = true
            ORDER BY p.created_at DESC
            LIMIT 20
        """)
        
        results["posts_with_media"] = [
            {
                "id": str(row["id"]),
                "channel_id": str(row["channel_id"]),
                "telegram_message_id": row["telegram_message_id"],
                "content_length": len(row["content"] or ""),
                "has_media": row["has_media"],
                "media_count": row["media_count"] or 0,
                "legacy_media_count": row["legacy_media_count"] or 0,
                "created_at": row["created_at"].isoformat() if row["created_at"] else None,
                "is_album_candidate": (row["media_count"] or 0) > 1
            }
            for row in posts_with_media
        ]
        
        # 2. –ü–æ—Å—Ç—ã —Å –∫–æ—Ä–æ—Ç–∫–∏–º —Ç–µ–∫—Å—Ç–æ–º + –º–µ–¥–∏–∞ (–¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–æ–≤–æ–π –ª–æ–≥–∏–∫–∏ tagging)
        MIN_CHARS = int(os.getenv("TAGGING_MIN_CHARS", "80"))
        posts_short_text_with_media = await conn.fetch("""
            SELECT 
                p.id,
                p.channel_id,
                p.content,
                LENGTH(p.content) as text_length,
                (SELECT COUNT(*) FROM post_media_map pmm WHERE pmm.post_id = p.id) as media_count
            FROM posts p
            WHERE p.has_media = true
            AND LENGTH(COALESCE(p.content, '')) < $1
            ORDER BY p.created_at DESC
            LIMIT 10
        """, MIN_CHARS)
        
        results["posts_short_text_with_media"] = [
            {
                "id": str(row["id"]),
                "channel_id": str(row["channel_id"]),
                "text_length": row["text_length"],
                "media_count": row["media_count"] or 0,
                "content_preview": (row["content"] or "")[:100]
            }
            for row in posts_short_text_with_media
        ]
        
        # 3. –ü–æ—Å—Ç—ã —Å Vision —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        posts_with_vision = await conn.fetch("""
            SELECT 
                p.id,
                p.channel_id,
                pe.vision_provider,
                pe.vision_analyzed_at,
                pe.vision_description,
                pe.vision_ocr_text,
                (SELECT COUNT(*) FROM post_media_map pmm WHERE pmm.post_id = p.id) as media_count
            FROM posts p
            JOIN post_enrichment pe ON pe.post_id = p.id
            WHERE pe.vision_analyzed_at IS NOT NULL
            ORDER BY pe.vision_analyzed_at DESC
            LIMIT 10
        """)
        
        results["posts_with_vision"] = [
            {
                "id": str(row["id"]),
                "channel_id": str(row["channel_id"]),
                "provider": row["vision_provider"],
                "analyzed_at": row["vision_analyzed_at"].isoformat() if row["vision_analyzed_at"] else None,
                "has_description": bool(row["vision_description"]),
                "has_ocr": bool(row["vision_ocr_text"]),
                "media_count": row["media_count"] or 0
            }
            for row in posts_with_vision
        ]
        
        # 4. Media objects —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        media_stats = await conn.fetchrow("""
            SELECT 
                COUNT(*) as total_objects,
                COUNT(DISTINCT mime) as unique_mimes,
                SUM(size_bytes) as total_size_bytes,
                AVG(size_bytes) as avg_size_bytes
            FROM media_objects
        """)
        
        results["media_statistics"] = {
            "total_objects": media_stats["total_objects"],
            "unique_mimes": media_stats["unique_mimes"],
            "total_size_gb": (media_stats["total_size_bytes"] or 0) / (1024**3),
            "avg_size_kb": (media_stats["avg_size_bytes"] or 0) / 1024
        }
        
        # 5. Post-media-map —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ (–Ω–æ–≤—ã–π —Å–ø–æ—Å–æ–± —Ö—Ä–∞–Ω–µ–Ω–∏—è)
        map_stats = await conn.fetchrow("""
            SELECT 
                COUNT(*) as total_links,
                COUNT(DISTINCT post_id) as posts_with_media,
                COUNT(DISTINCT file_sha256) as unique_media_files,
                COUNT(*) FILTER (WHERE role = 'primary') as primary_media,
                COUNT(*) FILTER (WHERE role = 'attachment') as attachment_media
            FROM post_media_map
        """)
        
        results["post_media_map_statistics"] = {
            "total_links": map_stats["total_links"],
            "posts_with_media": map_stats["posts_with_media"],
            "unique_media_files": map_stats["unique_media_files"],
            "primary_media": map_stats["primary_media"],
            "attachment_media": map_stats["attachment_media"]
        }
        
        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print(f"\n‚úÖ –ü–æ—Å—Ç–æ–≤ —Å –º–µ–¥–∏–∞: {len(results['posts_with_media'])}")
        print(f"   –ê–ª—å–±–æ–º—ã (candidates): {sum(1 for p in results['posts_with_media'] if p['is_album_candidate'])}")
        print(f"\n‚úÖ –ü–æ—Å—Ç–æ–≤ —Å –∫–æ—Ä–æ—Ç–∫–∏–º —Ç–µ–∫—Å—Ç–æ–º + –º–µ–¥–∏–∞: {len(results['posts_short_text_with_media'])}")
        print(f"   (—Ç–µ–∫—Å—Ç < {MIN_CHARS} —Å–∏–º–≤–æ–ª–æ–≤, –Ω–æ –µ—Å—Ç—å –º–µ–¥–∏–∞)")
        print(f"\n‚úÖ –ü–æ—Å—Ç–æ–≤ —Å Vision —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏: {len(results['posts_with_vision'])}")
        print(f"\nüìä Media Objects:")
        print(f"   –í—Å–µ–≥–æ: {results['media_statistics']['total_objects']}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö MIME: {results['media_statistics']['unique_mimes']}")
        print(f"   –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {results['media_statistics']['total_size_gb']:.2f} GB")
        print(f"\nüìä Post-Media-Map:")
        print(f"   –í—Å–µ–≥–æ —Å–≤—è–∑–µ–π: {results['post_media_map_statistics']['total_links']}")
        print(f"   –ü–æ—Å—Ç–æ–≤ —Å –º–µ–¥–∏–∞: {results['post_media_map_statistics']['posts_with_media']}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –º–µ–¥–∏–∞: {results['post_media_map_statistics']['unique_media_files']}")
        
        return results
        
    finally:
        await conn.close()


async def test_full_pipeline_for_post(post_id: UUID) -> Dict[str, Any]:
    """
    Context7: –ü–æ–ª–Ω—ã–π E2E —Ç–µ—Å—Ç –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–æ—Å—Ç–∞.
    
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤—Å–µ —ç—Ç–∞–ø—ã:
    1. –ù–∞–ª–∏—á–∏–µ –º–µ–¥–∏–∞ –≤ –ë–î (media_objects, post_media_map)
    2. Vision —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–µ—Å–ª–∏ –µ—Å—Ç—å)
    3. Tagging —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —É—á–µ—Ç–æ–º Vision
    4. –°–æ–±—ã—Ç–∏—è posts.parsed —Å media_sha256_list
    """
    print("\n" + "=" * 80)
    print(f"üß™ –ü–û–õ–ù–´–ô E2E –¢–ï–°–¢ –ü–ê–ô–ü–õ–ê–ô–ù–ê –î–õ–Ø POST_ID: {post_id}")
    print("=" * 80)
    
    conn = await asyncpg.connect(get_db_connection_string())
    redis_url = os.getenv("REDIS_URL", "redis://redis:6379")
    redis_client = redis.from_url(redis_url, decode_responses=False)
    
    try:
        results = {
            "post_id": str(post_id),
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "stages": {}
        }
        
        # –®–∞–≥ 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Å—Ç–∞ –≤ –ë–î
        print("\nüìã –®–∞–≥ 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Å—Ç–∞ –≤ –ë–î...")
        post_data = await conn.fetchrow("""
            SELECT 
                p.id,
                p.channel_id,
                p.telegram_message_id,
                p.content,
                p.has_media,
                p.created_at,
                LENGTH(p.content) as text_length
            FROM posts p
            WHERE p.id = $1
        """, post_id)
        
        if not post_data:
            results["error"] = f"–ü–æ—Å—Ç {post_id} –Ω–µ –Ω–∞–π–¥–µ–Ω"
            print(f"‚ùå {results['error']}")
            return results
        
        results["stages"]["post_found"] = {
            "channel_id": str(post_data["channel_id"]),
            "telegram_message_id": post_data["telegram_message_id"],
            "has_media": post_data["has_media"],
            "text_length": post_data["text_length"],
            "created_at": post_data["created_at"].isoformat() if post_data["created_at"] else None
        }
        print(f"‚úÖ –ü–æ—Å—Ç –Ω–∞–π–¥–µ–Ω: channel_id={post_data['channel_id']}, has_media={post_data['has_media']}")
        
        # –®–∞–≥ 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–µ–¥–∏–∞ –≤ post_media_map (–Ω–æ–≤—ã–π —Å–ø–æ—Å–æ–±)
        print("\nüìã –®–∞–≥ 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–µ–¥–∏–∞ –≤ post_media_map...")
        media_files = await conn.fetch("""
            SELECT 
                mo.file_sha256,
                mo.mime,
                mo.size_bytes,
                mo.s3_key,
                pmm.position,
                pmm.role
            FROM post_media_map pmm
            JOIN media_objects mo ON pmm.file_sha256 = mo.file_sha256
            WHERE pmm.post_id = $1
            ORDER BY pmm.position
        """, post_id)
        
        results["stages"]["media_files"] = [
            {
                "sha256": row["file_sha256"],
                "mime": row["mime"],
                "size_bytes": row["size_bytes"],
                "size_kb": row["size_bytes"] / 1024,
                "position": row["position"],
                "role": row["role"],
                "s3_key": row["s3_key"]
            }
            for row in media_files
        ]
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –º–µ–¥–∏–∞ —Ñ–∞–π–ª–æ–≤: {len(media_files)}")
        if media_files:
            for i, mf in enumerate(results["stages"]["media_files"]):
                print(f"   {i+1}. {mf['mime']} ({mf['size_kb']:.1f} KB) - {mf['sha256'][:16]}...")
        
        # –®–∞–≥ 3: –ü—Ä–æ–≤–µ—Ä–∫–∞ Vision —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("\nüìã –®–∞–≥ 3: –ü—Ä–æ–≤–µ—Ä–∫–∞ Vision —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        vision_data = await conn.fetchrow("""
            SELECT 
                pe.vision_provider,
                pe.vision_model,
                pe.vision_analyzed_at,
                pe.vision_description,
                pe.vision_ocr_text,
                pe.vision_is_meme,
                pe.vision_tokens_used,
                pe.data->>'description' as data_description,
                pe.data->>'ocr_text' as data_ocr
            FROM post_enrichment pe
            WHERE pe.post_id = $1 AND pe.kind = 'vision'
        """, post_id)
        
        if vision_data:
            results["stages"]["vision_analysis"] = {
                "provider": vision_data["vision_provider"],
                "model": vision_data["vision_model"],
                "analyzed_at": vision_data["vision_analyzed_at"].isoformat() if vision_data["vision_analyzed_at"] else None,
                "has_description": bool(vision_data["vision_description"] or vision_data["data_description"]),
                "has_ocr": bool(vision_data["vision_ocr_text"] or vision_data["data_ocr"]),
                "is_meme": vision_data["vision_is_meme"],
                "tokens_used": vision_data["vision_tokens_used"],
                "description_preview": (vision_data["vision_description"] or vision_data["data_description"] or "")[:200],
                "ocr_preview": (vision_data["vision_ocr_text"] or vision_data["data_ocr"] or "")[:200]
            }
            print(f"‚úÖ Vision –∞–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω:")
            print(f"   Provider: {vision_data['vision_provider']}")
            print(f"   Description: {'‚úÖ' if results['stages']['vision_analysis']['has_description'] else '‚ùå'}")
            print(f"   OCR: {'‚úÖ' if results['stages']['vision_analysis']['has_ocr'] else '‚ùå'}")
            if results["stages"]["vision_analysis"]["description_preview"]:
                print(f"   Preview: {results['stages']['vision_analysis']['description_preview'][:100]}...")
        else:
            results["stages"]["vision_analysis"] = None
            print("‚ö†Ô∏è  Vision –∞–Ω–∞–ª–∏–∑ –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # –®–∞–≥ 4: –ü—Ä–æ–≤–µ—Ä–∫–∞ Tagging —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("\nüìã –®–∞–≥ 4: –ü—Ä–æ–≤–µ—Ä–∫–∞ Tagging —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        tagging_data = await conn.fetchrow("""
            SELECT 
                pe.provider,
                pe.data->>'tags' as tags_json,
                pe.tags as legacy_tags,
                pe.created_at
            FROM post_enrichment pe
            WHERE pe.post_id = $1 AND pe.kind = 'tags'
            ORDER BY pe.created_at DESC
            LIMIT 1
        """, post_id)
        
        if tagging_data:
            # –ü–∞—Ä—Å–∏–º —Ç–µ–≥–∏ –∏–∑ data –∏–ª–∏ legacy –ø–æ–ª—è
            import json as json_lib
            try:
                tags = json_lib.loads(tagging_data["tags_json"]) if tagging_data["tags_json"] else []
            except:
                tags = tagging_data["legacy_tags"] if tagging_data["legacy_tags"] else []
            
            results["stages"]["tagging"] = {
                "provider": tagging_data["provider"],
                "tags_count": len(tags) if isinstance(tags, list) else 0,
                "tags": tags[:10] if isinstance(tags, list) else [],
                "created_at": tagging_data["created_at"].isoformat() if tagging_data["created_at"] else None
            }
            print(f"‚úÖ Tagging –≤—ã–ø–æ–ª–Ω–µ–Ω:")
            print(f"   Provider: {tagging_data['provider']}")
            print(f"   –¢–µ–≥–æ–≤: {results['stages']['tagging']['tags_count']}")
            if results["stages"]["tagging"]["tags"]:
                print(f"   –ü—Ä–∏–º–µ—Ä—ã: {', '.join(results['stages']['tagging']['tags'][:5])}")
        else:
            results["stages"]["tagging"] = None
            print("‚ö†Ô∏è  Tagging –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω")
        
        # –®–∞–≥ 5: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–±—ã—Ç–∏–π posts.parsed —Å media_sha256_list
        print("\nüìã –®–∞–≥ 5: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–±—ã—Ç–∏–π posts.parsed...")
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º Redis Stream –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–æ–±—ã—Ç–∏–π
        try:
            stream_name = "stream:posts:parsed"
            stream_length = await redis_client.xlen(stream_name)
            print(f"‚úÖ Stream posts:parsed: {stream_length} —Å–æ–±—ã—Ç–∏–π")
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–±—ã—Ç–∏—è
            last_events = await redis_client.xrevrange(stream_name, count=100)
            
            # –ò—â–µ–º —Å–æ–±—ã—Ç–∏–µ –¥–ª—è –Ω–∞—à–µ–≥–æ –ø–æ—Å—Ç–∞
            post_event_found = False
            for event_id, event_data in last_events:
                try:
                    if isinstance(event_data.get(b'data'), bytes):
                        event_json = json.loads(event_data[b'data'].decode())
                    else:
                        event_json = json.loads(event_data.get('data', '{}'))
                    
                    if event_json.get('post_id') == str(post_id):
                        post_event_found = True
                        media_sha256_list = event_json.get('media_sha256_list', [])
                        results["stages"]["parsed_event"] = {
                            "event_id": event_id.decode() if isinstance(event_id, bytes) else event_id,
                            "has_media_sha256_list": bool(media_sha256_list),
                            "media_sha256_count": len(media_sha256_list),
                            "media_sha256_list": media_sha256_list[:5]  # –ü–µ—Ä–≤—ã–µ 5
                        }
                        print(f"‚úÖ –°–æ–±—ã—Ç–∏–µ posts.parsed –Ω–∞–π–¥–µ–Ω–æ:")
                        print(f"   Event ID: {results['stages']['parsed_event']['event_id']}")
                        print(f"   Media SHA256 –≤ —Å–æ–±—ã—Ç–∏–∏: {len(media_sha256_list)}")
                        break
                except Exception as e:
                    continue
            
            if not post_event_found:
                results["stages"]["parsed_event"] = None
                print("‚ö†Ô∏è  –°–æ–±—ã—Ç–∏–µ posts.parsed –¥–ª—è —ç—Ç–æ–≥–æ –ø–æ—Å—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 100 —Å–æ–±—ã—Ç–∏—è—Ö")
                
        except Exception as e:
            results["stages"]["parsed_event"] = {"error": str(e)}
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–±—ã—Ç–∏–π: {e}")
        
        # –®–∞–≥ 6: –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞
        print("\nüìã –®–∞–≥ 6: –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞...")
        validation_results = {
            "media_processed": len(media_files) > 0,
            "vision_completed": results["stages"]["vision_analysis"] is not None,
            "tagging_completed": results["stages"]["tagging"] is not None,
            "event_has_media_sha256": results["stages"].get("parsed_event", {}).get("has_media_sha256_list", False),
            "short_text_with_media": post_data["text_length"] < int(os.getenv("TAGGING_MIN_CHARS", "80")) and post_data["has_media"]
        }
        
        results["stages"]["validation"] = validation_results
        
        print("\n‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è:")
        print(f"   –ú–µ–¥–∏–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã: {'‚úÖ' if validation_results['media_processed'] else '‚ùå'}")
        print(f"   Vision –≤—ã–ø–æ–ª–Ω–µ–Ω: {'‚úÖ' if validation_results['vision_completed'] else '‚ùå'}")
        print(f"   Tagging –≤—ã–ø–æ–ª–Ω–µ–Ω: {'‚úÖ' if validation_results['tagging_completed'] else '‚ùå'}")
        print(f"   Event —Å–æ–¥–µ—Ä–∂–∏—Ç media_sha256_list: {'‚úÖ' if validation_results['event_has_media_sha256'] else '‚ùå'}")
        print(f"   –ö–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç + –º–µ–¥–∏–∞: {'‚úÖ' if validation_results['short_text_with_media'] else '‚ùå'}")
        
        if all([
            validation_results["media_processed"],
            validation_results["vision_completed"],
            validation_results["tagging_completed"],
            validation_results["event_has_media_sha256"]
        ]):
            results["success"] = True
            print("\n" + "=" * 80)
            print("‚úÖ –í–°–ï –≠–¢–ê–ü–´ –ü–ê–ô–ü–õ–ê–ô–ù–ê –£–°–ü–ï–®–ù–û –ü–†–û–ô–î–ï–ù–´!")
            print("=" * 80)
        else:
            results["success"] = False
            print("\n‚ö†Ô∏è  –ù–µ –≤—Å–µ —ç—Ç–∞–ø—ã –ø–∞–π–ø–ª–∞–π–Ω–∞ –∑–∞–≤–µ—Ä—à–µ–Ω—ã")
        
        return results
        
    finally:
        await conn.close()
        await redis_client.close()


async def main():
    """Context7: –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""
    parser = argparse.ArgumentParser(
        description="–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤–æ–≤–≤–µ–¥–µ–Ω–∏–π –∞—É–¥–∏—Ç–∞ –º–µ–¥–∏–∞ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
  python scripts/test_media_audit_features.py --check-real-data
  
  # –¢–µ—Å—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–æ—Å—Ç–∞
  python scripts/test_media_audit_features.py --test-post-id <uuid>
  
  # –ü–æ–ª–Ω—ã–π —Ç–µ—Å—Ç –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è –ø–æ—Å—Ç–∞ —Å –º–µ–¥–∏–∞
  python scripts/test_media_audit_features.py --test-post-id <uuid> --full
        """
    )
    parser.add_argument("--check-real-data", action="store_true", 
                       help="–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
    parser.add_argument("--test-post-id", type=str, 
                       help="UUID –ø–æ—Å—Ç–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
    parser.add_argument("--full", action="store_true",
                       help="–ü–æ–ª–Ω—ã–π —Ç–µ—Å—Ç –ø–∞–π–ø–ª–∞–π–Ω–∞")
    
    args = parser.parse_args()
    
    if args.check_real_data:
        results = await check_real_data_status()
        print("\n" + "=" * 80)
        print("üìã –ò–¢–û–ì–û–í–´–ô –û–¢–ß–Å–¢")
        print("=" * 80)
        print(json.dumps(results, indent=2, default=str, ensure_ascii=False))
    
    elif args.test_post_id:
        post_id = UUID(args.test_post_id)
        results = await test_full_pipeline_for_post(post_id)
        print("\n" + "=" * 80)
        print("üìã –ò–¢–û–ì–û–í–´–ô –û–¢–ß–Å–¢")
        print("=" * 80)
        print(json.dumps(results, indent=2, default=str, ensure_ascii=False))
    
    else:
        parser.print_help()


if __name__ == "__main__":
    asyncio.run(main())

