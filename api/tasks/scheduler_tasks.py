"""
Scheduled tasks –¥–ª—è –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–π–¥–∂–µ—Å—Ç–æ–≤ –∏ –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤.
Context7: –ò—Å–ø–æ–ª—å–∑—É–µ–º APScheduler –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–¥–∞—á
"""

import asyncio
from datetime import datetime, date, time, timezone
from typing import List, Optional
import structlog
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger
from prometheus_client import Counter
from sqlalchemy.orm import Session

from models.database import get_db, DigestSettings, User, TrendDetection, DigestHistory, UserInterest
from sqlalchemy import and_
from services.trend_detection_service import get_trend_detection_service
from services.user_interest_service import get_user_interest_service
from services.graph_service import get_graph_service
from config import settings
from middleware.rls_middleware import set_tenant_id_in_session
from worker.event_bus import EventPublisher, RedisStreamsClient, DigestGenerateEvent
from uuid import UUID

logger = structlog.get_logger()

digest_retry_total = Counter(
    'digest_retry_total',
    '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫ –æ—Ç–ø—Ä–∞–≤–∫–∏ –¥–∞–π–¥–∂–µ—Å—Ç–∞',
    ['tenant_id']
)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π scheduler
scheduler: AsyncIOScheduler = None

_digest_event_publisher: Optional[EventPublisher] = None
_digest_publisher_lock: asyncio.Lock = asyncio.Lock()


def init_scheduler():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è APScheduler."""
    global scheduler
    if scheduler is None:
        scheduler = AsyncIOScheduler()
        logger.info("APScheduler initialized")
    return scheduler


async def _get_digest_event_publisher() -> EventPublisher:
    """
    –õ–µ–Ω–∏–≤–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –ø—É–±–ª–∏–∫–∞—Ç–æ—Ä–∞ —Å–æ–±—ã—Ç–∏–π –¥–ª—è –æ—á–µ—Ä–µ–¥–∏ –¥–∞–π–¥–∂–µ—Å—Ç–æ–≤.
    
    Context7: –æ–¥–∏–Ω —ç–∫–∑–µ–º–ø–ª—è—Ä –Ω–∞ –ø—Ä–æ—Ü–µ—Å—Å, –∑–∞—â–∏—â—ë–Ω–Ω—ã–π asyncio.Lock.
    """
    global _digest_event_publisher
    if _digest_event_publisher is not None:
        return _digest_event_publisher
    
    async with _digest_publisher_lock:
        if _digest_event_publisher is None:
            redis_url = getattr(settings, "redis_url", "redis://redis:6379")
            client = RedisStreamsClient(redis_url)
            await client.connect()
            _digest_event_publisher = EventPublisher(client)
            logger.info("Digest event publisher initialized", redis_url=redis_url)
    return _digest_event_publisher


async def generate_digest_for_user(
    user_id: str,
    tenant_id: str,
    topics: List[str],
    db: Session,
    trigger: str = "scheduler",
    requested_by: Optional[str] = None
) -> Optional[DigestHistory]:
    """
    –ü–ª–∞–Ω–∏—Ä—É–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –¥–∞–π–¥–∂–µ—Å—Ç–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —á–µ—Ä–µ–∑ –æ—á–µ—Ä–µ–¥—å.
    
    Context7: fail-fast –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ tenant_id –∏–ª–∏ —Ç–µ–º.
    """
    if not tenant_id:
        logger.warning("Cannot enqueue digest without tenant_id", user_id=user_id)
        return None
    
    if not topics:
        logger.debug("Skipping digest enqueue for user without topics", user_id=user_id)
        return None
    
    try:
        if settings.feature_rls_enabled:
            set_tenant_id_in_session(db, tenant_id)
        
        today = date.today()
        user_uuid = UUID(user_id)
        tenant_uuid = UUID(tenant_id)
        
        existing = db.query(DigestHistory).filter(
            and_(
                DigestHistory.user_id == user_uuid,
                DigestHistory.digest_date == today
            )
        ).order_by(DigestHistory.created_at.desc()).first()
        
        force_new = trigger == "manual"

        if existing:
            if existing.status in {"scheduled", "pending", "processing"}:
                if force_new:
                    logger.warning(
                        "Manual trigger overriding in-flight digest",
                        user_id=user_id,
                        digest_id=str(existing.id),
                        status=existing.status,
                    )
                    existing.status = "failed"
                    existing.sent_at = None
                    db.commit()
                    existing = None
                else:
                    logger.debug(
                        "Digest already scheduled or in progress",
                        user_id=user_id,
                        digest_id=str(existing.id),
                        status=existing.status
                    )
                    return existing

            if existing and existing.status == "sent":
                if not force_new:
                    logger.debug(
                        "Digest already sent for today, returning existing",
                        user_id=user_id,
                        digest_id=str(existing.id),
                    )
                    return existing
                else:
                    logger.info(
                        "Manual trigger: creating fresh digest despite existing sent record",
                        user_id=user_id,
                        previous_digest_id=str(existing.id),
                    )
                    existing = None
        
        if existing and existing.status == "failed":
            digest_history = existing
            digest_history.status = "pending"
            digest_history.content = digest_history.content or ""
            digest_history.posts_count = digest_history.posts_count or 0
            digest_history.topics = topics
            digest_history.tenant_id = tenant_uuid
            db.commit()
            db.refresh(digest_history)
            logger.info(
                "Re-scheduling failed digest",
                user_id=user_id,
                digest_id=str(digest_history.id)
            )
        else:
            digest_history = DigestHistory(
                user_id=user_uuid,
                tenant_id=tenant_uuid,
                digest_date=today,
                content="",
                posts_count=0,
                topics=topics,
                status="pending"
            )
            db.add(digest_history)
            db.commit()
            db.refresh(digest_history)
            logger.info(
                "Digest placeholder created",
                user_id=user_id,
                digest_id=str(digest_history.id)
            )
        
        event = DigestGenerateEvent(
            idempotency_key=f"digest:{user_id}:{today.isoformat()}",
            user_id=user_id,
            tenant_id=tenant_id,
            digest_date=today,
            history_id=str(digest_history.id),
            trigger=trigger,
            requested_by=requested_by
        )
        
        publisher = await _get_digest_event_publisher()
        await publisher.publish_event("digests.generate", event)
        
        digest_history.status = "pending"
        digest_history.topics = topics
        db.commit()
        
        logger.info(
            "Digest generation enqueued",
            user_id=user_id,
            tenant_id=tenant_id,
            digest_id=str(digest_history.id),
            trigger=trigger
        )
        return digest_history
    
    except Exception as e:
        logger.error(
            "Error enqueueing digest generation",
            user_id=user_id,
            tenant_id=tenant_id,
            error=str(e)
        )
        raise


async def process_digests_task():
    """
    –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–π–¥–∂–µ—Å—Ç–æ–≤.
    
    Context7: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å –≤–∫–ª—é—á–µ–Ω–Ω—ã–º–∏ –¥–∞–π–¥–∂–µ—Å—Ç–∞–º–∏ –∏ topics,
    –≤—ã—á–∏—Å–ª—è–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –ø–æ schedule_tz –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–∞–π–¥–∂–µ—Å—Ç—ã.
    """
    try:
        from pytz import timezone as pytz_timezone
        
        db = next(get_db())
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å –≤–∫–ª—é—á–µ–Ω–Ω—ã–º–∏ –¥–∞–π–¥–∂–µ—Å—Ç–∞–º–∏
        digest_settings = db.query(DigestSettings).filter(
            DigestSettings.enabled == True
        ).all()
        
        current_utc = datetime.now(timezone.utc)
        
        for setting in digest_settings:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ topics
                if not setting.topics or len(setting.topics) == 0:
                    logger.debug("Skipping user without topics", user_id=str(setting.user_id))
                    continue
                
                # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è tenant_id
                user = db.query(User).filter(User.id == setting.user_id).first()
                if not user:
                    continue
                
                if not user.tenant_id:
                    logger.warning(
                        "Skipping digest scheduling due to missing tenant_id",
                        user_id=str(setting.user_id)
                    )
                    continue

                tenant_id = str(user.tenant_id)
                
                if settings.feature_rls_enabled:
                    set_tenant_id_in_session(db, tenant_id)
                
                # –í—ã—á–∏—Å–ª—è–µ–º –ª–æ–∫–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                user_tz = pytz_timezone(setting.schedule_tz)
                local_time = current_utc.astimezone(user_tz).time()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–∞–π–¥–∂–µ—Å—Ç —Å–µ–π—á–∞—Å
                schedule_time = setting.schedule_time
                if isinstance(schedule_time, str):
                    schedule_time = time.fromisoformat(schedule_time)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏ —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é
                # (—Ç–∏–∫ –∫–∞–∂–¥—ã–µ 15 –º–∏–Ω—É—Ç, –ø—Ä–æ–≤–µ—Ä—è–µ–º –æ–∫–Ω–æ ¬±5 –º–∏–Ω—É—Ç)
                time_diff = abs(
                    (local_time.hour * 60 + local_time.minute) -
                    (schedule_time.hour * 60 + schedule_time.minute)
                )
                
                if time_diff <= 5:  # –í –ø—Ä–µ–¥–µ–ª–∞—Ö 5 –º–∏–Ω—É—Ç –æ—Ç —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –±—ã–ª –ª–∏ —É–∂–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –¥–∞–π–¥–∂–µ—Å—Ç —Å–µ–≥–æ–¥–Ω—è
                    from datetime import date
                    today = date.today()
                    existing = db.query(DigestHistory).filter(
                        and_(
                            DigestHistory.user_id == setting.user_id,
                            DigestHistory.digest_date == today
                        )
                    ).order_by(DigestHistory.created_at.desc()).first()

                    retry_cooldown = getattr(settings, "digest_retry_cooldown_min", 15)
                    
                    if existing:
                        if existing.status == "sent":
                            continue

                        if existing.status in {"scheduled", "pending", "processing"}:
                            logger.debug(
                                "Digest already queued, skipping duplicate",
                                user_id=str(setting.user_id),
                                digest_id=str(existing.id),
                                status=existing.status
                            )
                            continue

                        if existing.created_at:
                            created_at = existing.created_at
                            if created_at.tzinfo is None:
                                created_at = created_at.replace(tzinfo=timezone.utc)
                            age_minutes = (current_utc - created_at).total_seconds() / 60.0
                        else:
                            age_minutes = float("inf")

                        if age_minutes < retry_cooldown:
                            logger.debug(
                                "Skip digest retry due to cooldown",
                                user_id=str(setting.user_id),
                                status=existing.status,
                                age_minutes=age_minutes
                            )
                            continue

                        logger.info(
                            "Re-enqueueing failed digest generation",
                            user_id=str(setting.user_id),
                            digest_id=str(existing.id),
                            status=existing.status
                        )
                        digest_retry_total.labels(tenant_id=tenant_id).inc()
                        await generate_digest_for_user(
                            user_id=str(setting.user_id),
                            tenant_id=tenant_id,
                            topics=setting.topics,
                            db=db,
                            trigger="scheduler_retry"
                        )
                    else:
                        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–π–¥–∂–µ—Å—Ç
                        await generate_digest_for_user(
                            user_id=str(setting.user_id),
                            tenant_id=tenant_id,
                            topics=setting.topics,
                            db=db,
                            trigger="scheduler"
                        )
            
            except Exception as e:
                logger.error("Error processing digest for user", user_id=str(setting.user_id), error=str(e))
                continue
        
        db.close()
        logger.info("Digest processing task completed")
    
    except Exception as e:
        logger.error("Error in digest processing task", error=str(e))


async def detect_trends_task():
    """
    –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤.
    
    Context7: –ï–∂–µ–¥–Ω–µ–≤–Ω—ã–π –±–∞—Ç—á –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–æ–≤ –∏–∑ –≤—Å–µ—Ö –ø–æ—Å—Ç–æ–≤.
    """
    try:
        db = next(get_db())
        
        trend_service = get_trend_detection_service()
        
        trends = await trend_service.detect_trends(
            days=7,
            min_frequency=10,
            min_growth=0.2,
            min_engagement=5.0,
            db=db
        )
        
        logger.info(
            "Trend detection task completed",
            trends_count=len(trends)
        )
        
        # –û—Ç–ø—Ä–∞–≤–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π –æ —Ç—Ä–µ–Ω–¥–∞—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
        if trends:
            await send_trend_alerts_to_users(trends, db)
        
        db.close()
    
    except Exception as e:
        logger.error("Error in trend detection task", error=str(e))


async def send_trend_alerts_to_users(trends: List, db: Session):
    """
    –û—Ç–ø—Ä–∞–≤–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π –æ —Ç—Ä–µ–Ω–¥–∞—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º —á–µ—Ä–µ–∑ Telegram.
    
    Context7: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∫–∞–∫–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –ø–æ–¥–ø–∏—Å–∞–Ω—ã –Ω–∞ –∫–∞–Ω–∞–ª—ã/—Ç–µ–º—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —Ç—Ä–µ–Ω–¥–∞–º–∏,
    –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è —á–µ—Ä–µ–∑ –±–æ—Ç–∞.
    """
    try:
        from models.database import User, TrendAlert, UserChannel, Channel
        from bot.webhook import bot
        from datetime import timezone
        
        if not bot:
            logger.warning("Bot not initialized, cannot send trend alerts")
            return
        
        alerts_sent = 0
        
        for trend_result in trends:
            trend_id = trend_result.trend_id
            trend_keyword = trend_result.keyword
            channels_affected = trend_result.channels_affected
            
            # –ù–∞—Ö–æ–¥–∏–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, –ø–æ–¥–ø–∏—Å–∞–Ω–Ω—ã—Ö –Ω–∞ –∑–∞—Ç—Ä–æ–Ω—É—Ç—ã–µ –∫–∞–Ω–∞–ª—ã
            if channels_affected:
                # –ü–æ–ª—É—á–∞–µ–º channel_id –∏–∑ channels_affected (—Å–ø–∏—Å–æ–∫ UUID —Å—Ç—Ä–æ–∫)
                from uuid import UUID
                channel_ids = [UUID(cid) for cid in channels_affected if cid]
                
                # –ù–∞—Ö–æ–¥–∏–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, –ø–æ–¥–ø–∏—Å–∞–Ω–Ω—ã—Ö –Ω–∞ —ç—Ç–∏ –∫–∞–Ω–∞–ª—ã
                subscribed_users = db.query(User).join(UserChannel).filter(
                    UserChannel.channel_id.in_(channel_ids)
                ).distinct().all()
                
                for user in subscribed_users:
                    if not user.telegram_id:
                        continue
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –±—ã–ª–æ –ª–∏ —É–∂–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
                    existing_alert = db.query(TrendAlert).filter(
                        TrendAlert.user_id == user.id,
                        TrendAlert.trend_id == trend_id
                    ).first()
                    
                    if existing_alert:
                        continue  # –£–∂–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ
                    
                    try:
                        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ —Ç—Ä–µ–Ω–¥–µ
                        trend_message = f"üìà <b>–ù–æ–≤—ã–π —Ç—Ä–µ–Ω–¥ –æ–±–Ω–∞—Ä—É–∂–µ–Ω!</b>\n\n"
                        trend_message += f"<b>–¢–µ–º–∞:</b> {trend_keyword}\n"
                        trend_message += f"<b>–ß–∞—Å—Ç–æ—Ç–∞ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π:</b> {trend_result.frequency}\n"
                        
                        if trend_result.engagement_score:
                            trend_message += f"<b>Engagement:</b> {trend_result.engagement_score:.1f}\n"
                        
                        if channels_affected:
                            # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–∞–Ω–∞–ª–æ–≤
                            channels = db.query(Channel).filter(
                                Channel.id.in_(channel_ids[:5])  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ 5 –∫–∞–Ω–∞–ª–æ–≤
                            ).all()
                            if channels:
                                trend_message += f"\n<b>–ó–∞—Ç—Ä–æ–Ω—É—Ç—ã–µ –∫–∞–Ω–∞–ª—ã:</b>\n"
                                for channel in channels[:5]:
                                    trend_message += f"‚Ä¢ {channel.title}\n"
                        
                        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
                        await bot.send_message(
                            chat_id=user.telegram_id,
                            text=trend_message,
                            parse_mode="HTML"
                        )
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
                        trend_alert = TrendAlert(
                            user_id=user.id,
                            trend_id=trend_id
                        )
                        db.add(trend_alert)
                        alerts_sent += 1
                        
                        logger.debug(
                            "Trend alert sent",
                            user_id=str(user.id),
                            trend_id=str(trend_id),
                            trend_keyword=trend_keyword
                        )
                    
                    except Exception as e:
                        logger.error(
                            "Error sending trend alert",
                            user_id=str(user.id),
                            trend_id=str(trend_id),
                            error=str(e)
                        )
                        continue
        
        if alerts_sent > 0:
            db.commit()
            logger.info("Trend alerts sent to users", alerts_count=alerts_sent, trends_count=len(trends))
    
    except Exception as e:
        logger.error("Error in send_trend_alerts_to_users", error=str(e))
        if 'db' in locals():
            db.rollback()


async def sync_user_interests_to_neo4j_task():
    """
    Context7: –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤ PostgreSQL ‚Üí Neo4j.
    
    –ß–∏—Ç–∞–µ—Ç –∏–Ω—Ç–µ—Ä–µ—Å—ã –∏–∑ PostgreSQL (source of truth) –∏ Redis (pending updates),
    —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ—Ç –≤ Neo4j (MERGE –æ–ø–µ—Ä–∞—Ü–∏—è) —Å –∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å—é.
    """
    try:
        db = next(get_db())
        graph_service = get_graph_service()
        user_interest_service = get_user_interest_service()
        
        # Context7: Health check –ø–µ—Ä–µ–¥ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–µ–π
        if not await graph_service.health_check():
            logger.warning("Neo4j unavailable, skipping interests sync")
            return
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å –∏–Ω—Ç–µ—Ä–µ—Å–∞–º–∏
        users_with_interests = db.query(UserInterest.user_id).distinct().all()
        
        synced_count = 0
        error_count = 0
        
        for (user_id,) in users_with_interests:
            try:
                # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ç–µ—Ä–µ—Å—ã –∏–∑ PostgreSQL
                interests = await user_interest_service.get_user_interests(user_id, limit=50, db=db)
                
                if not interests:
                    continue
                
                # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –∏–Ω—Ç–µ—Ä–µ—Å –≤ Neo4j
                for interest in interests:
                    topic = interest.get('topic')
                    weight = interest.get('weight', 0.0)
                    
                    if topic and weight > 0:
                        success = await graph_service.update_user_interest(
                            user_id=str(user_id),
                            topic=topic,
                            weight=weight
                        )
                        
                        if success:
                            synced_count += 1
                        else:
                            error_count += 1
                
                logger.debug("User interests synced to Neo4j", user_id=str(user_id), count=len(interests))
                
            except Exception as e:
                logger.error("Error syncing user interests to Neo4j", user_id=str(user_id), error=str(e))
                error_count += 1
        
        logger.info(
            "Interests sync completed",
            users_count=len(users_with_interests),
            synced_count=synced_count,
            error_count=error_count
        )
        
        # Context7: –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ —Å retry –∏ DLQ –¥–ª—è failed —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–π
        if error_count > 0:
            try:
                import redis.asyncio as redis
                redis_client = redis.from_url(settings.redis_url, decode_responses=True)
                
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º failed —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –≤ DLQ stream
                for _ in range(error_count):
                    await redis_client.xadd(
                        "stream:user_interests.sync.failed",
                        {
                            "timestamp": datetime.now(timezone.utc).isoformat(),
                            "error_count": str(error_count)
                        },
                        maxlen=1000  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ stream
                    )
                
                logger.info("Failed sync operations sent to DLQ", error_count=error_count)
            except Exception as dlq_error:
                logger.warning("Failed to send to DLQ", error=str(dlq_error))
        
    except Exception as e:
        logger.error("Error in sync_user_interests_to_neo4j_task", error=str(e))
        # Context7: DLQ –¥–ª—è failed —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–π
        try:
            import redis.asyncio as redis
            redis_client = redis.from_url(settings.redis_url, decode_responses=True)
            await redis_client.xadd(
                "stream:user_interests.sync.failed",
                {
                    "error": str(e),
                    "timestamp": datetime.now(timezone.utc).isoformat()
                },
                maxlen=1000
            )
        except Exception:
            pass  # –ù–µ –∫—Ä–∏—Ç–∏—á–Ω–æ, –µ—Å–ª–∏ DLQ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω


def setup_scheduled_tasks():
    """
    –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á.
    
    Context7:
    - –î–∞–π–¥–∂–µ—Å—Ç—ã: –∫–∞–∂–¥—ã–µ 15 –º–∏–Ω—É—Ç (–ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π)
    - –¢—Ä–µ–Ω–¥—ã: –µ–∂–µ–¥–Ω–µ–≤–Ω–æ –≤ 00:00 UTC
    - –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤: –∫–∞–∂–¥—ã–µ N –º–∏–Ω—É—Ç (–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫)
    """
    global scheduler
    
    if scheduler is None:
        scheduler = init_scheduler()
    
    # –î–∞–π–¥–∂–µ—Å—Ç—ã: –∫–∞–∂–¥—ã–µ 15 –º–∏–Ω—É—Ç
    scheduler.add_job(
        process_digests_task,
        trigger=CronTrigger(minute="*/15"),  # –ö–∞–∂–¥—ã–µ 15 –º–∏–Ω—É—Ç
        id="process_digests",
        name="Process user digests",
        replace_existing=True
    )
    
    # –¢—Ä–µ–Ω–¥—ã: –µ–∂–µ–¥–Ω–µ–≤–Ω–æ –≤ 00:00 UTC
    scheduler.add_job(
        detect_trends_task,
        trigger=CronTrigger(hour=0, minute=0),  # –ü–æ–ª–Ω–æ—á—å UTC
        id="detect_trends",
        name="Detect trends from all posts",
        replace_existing=True
    )
    
    # Context7: –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤ PostgreSQL ‚Üí Neo4j –∫–∞–∂–¥—ã–µ N –º–∏–Ω—É—Ç
    sync_interval = getattr(settings, 'neo4j_interest_sync_interval_min', 15)
    scheduler.add_job(
        sync_user_interests_to_neo4j_task,
        trigger=CronTrigger(minute=f'*/{sync_interval}'),
        id="sync_user_interests",
        name="Sync user interests to Neo4j",
        replace_existing=True
    )
    
    logger.info("Scheduled tasks configured", tasks=["process_digests", "detect_trends", "sync_user_interests"])


def start_scheduler():
    """–ó–∞–ø—É—Å–∫ scheduler."""
    global scheduler
    
    if scheduler is None:
        scheduler = init_scheduler()
    
    if not scheduler.running:
        scheduler.start()
        setup_scheduled_tasks()
        logger.info("Scheduler started")
    else:
        logger.warning("Scheduler already running")


def stop_scheduler():
    """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ scheduler."""
    global scheduler
    
    if scheduler and scheduler.running:
        scheduler.shutdown()
        logger.info("Scheduler stopped")

