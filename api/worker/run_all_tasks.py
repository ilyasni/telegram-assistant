#!/usr/bin/env python3
"""
–ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö worker tasks —Å supervisor pattern –¥–ª—è –∞–≤—Ç–æ–ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞.
"""

import asyncio
import sys
import os
import logging
from pathlib import Path

import importlib
import yaml

# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –∏ project root –≤ PYTHONPATH
CURRENT_DIR = Path(__file__).resolve().parent
sys.path.insert(0, str(CURRENT_DIR))

PROJECT_ROOT = Path("/opt/telegram-assistant")
PROJECT_API = PROJECT_ROOT / "api"
PROJECT_WORKER = PROJECT_ROOT / "worker"
TASKS_DIR = CURRENT_DIR / "tasks"

for candidate in (
    PROJECT_ROOT,
    PROJECT_API,
    PROJECT_WORKER,
    TASKS_DIR,
    PROJECT_API / "worker",
):
    try:
        if candidate.exists():
            sys.path.insert(0, str(candidate))
    except Exception:
        continue


def _import_task(module: str, attr: str | None = None):
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç —Å fallback:
    1. tasks.<module>
    2. worker.tasks.<module>
    """
    for prefix in ("tasks", "worker.tasks"):
        try:
            loaded = importlib.import_module(f"{prefix}.{module}")
            return getattr(loaded, attr) if attr else loaded
        except ModuleNotFoundError:
            continue
    loaded = importlib.import_module(module)  # –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ (–µ—Å–ª–∏ –ø—É—Ç—å –∞–±—Å–æ–ª—é—Ç–Ω—ã–π)
    return getattr(loaded, attr) if attr else loaded

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s'
)
logger = logging.getLogger(__name__)

from supervisor import TaskSupervisor, TaskConfig

TaggingTask = _import_task("tagging_task", "TaggingTask")
EnrichmentWorker = _import_task("enrichment_task", "EnrichmentWorker")
IndexingTask = _import_task("indexing_task", "IndexingTask")
TagPersistenceTask = _import_task("tag_persistence_task", "TagPersistenceTask")
CrawlTriggerTask = _import_task("crawl_trigger_task", "CrawlTriggerTask")
PostPersistenceWorker = _import_task("post_persistence_task", "PostPersistenceWorker")
RetaggingTask = _import_task("retagging_task", "RetaggingTask")
AlbumAssemblerTask = _import_task("album_assembler_task", "AlbumAssemblerTask")
TrendDetectionWorker = _import_task("trends_worker", "TrendDetectionWorker")
TrendEditorAgent = _import_task("trends_editor_agent", "TrendEditorAgent")
create_trend_editor_agent = _import_task("trends_editor_agent", "create_trend_editor_agent")

digest_worker = _import_task("digest_worker")
create_digest_worker_task = getattr(digest_worker, "create_digest_worker_task")
digest_jobs_processed_total = getattr(digest_worker, "digest_jobs_processed_total")
digest_worker_generation_seconds = getattr(digest_worker, "digest_worker_generation_seconds")
digest_worker_send_seconds = getattr(digest_worker, "digest_worker_send_seconds")

create_digest_context_task = _import_task("context_events_task", "create_digest_context_task")

from run_all_tasks_vision_helper import get_s3_config_from_env, get_vision_config_from_env

async def create_tagging_task():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ tagging task."""
    redis_url = os.getenv("REDIS_URL", "redis://redis:6379")
    # –ü–æ best practices TaggingTask –Ω–µ —Ç—Ä–µ–±—É–µ—Ç DATABASE_URL –∏ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Ç–æ–ª—å–∫–æ redis_url
    task = TaggingTask(redis_url)
    await task.start()
    # Context7: –ù–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –∏–∑ start() - –∑–∞–¥–∞—á–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –≤ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–º —Ü–∏–∫–ª–µ

async def create_enrichment_task():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ enrichment task."""
    # Enrichment task —Ç—Ä–µ–±—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
    from tasks.enrichment_task import main as enrichment_main
    await enrichment_main()

async def create_indexing_task():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ indexing task."""
    redis_url = os.getenv("REDIS_URL", "redis://redis:6379")
    qdrant_url = os.getenv("QDRANT_URL", "http://localhost:6333")
    neo4j_url = os.getenv("NEO4J_URL", "bolt://localhost:7687")
    
    task = IndexingTask(redis_url, qdrant_url, neo4j_url)
    await task.start()

async def create_tag_persistence_task():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ tag persistence task."""
    redis_url = os.getenv("REDIS_URL", "redis://redis:6379")
    database_url = os.getenv("DATABASE_URL", "postgresql://postgres:postgres@localhost:5432/postgres")
    
    task = TagPersistenceTask(redis_url, database_url)
    await task.start()

async def create_crawl_trigger_task():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ crawl trigger task."""
    try:
        logger.info("create_crawl_trigger_task: Starting initialization")
    except Exception as e:
        logger.error(f"create_crawl_trigger_task: Error in initial logging: {e}")
    
    redis_url = os.getenv("REDIS_URL", "redis://redis:6379")
    database_url = os.getenv("DATABASE_URL", "postgresql://postgres:postgres@localhost:5432/postgres")

    default_trigger_tags = [
        'longread', 'research', 'paper', 'release', 'law',
        'deepdive', 'analysis', 'report', 'study', 'whitepaper'
    ]
    trigger_tags = default_trigger_tags

    try:
        logger.info("create_crawl_trigger_task: Loading config")
    except Exception as e:
        logger.error(f"create_crawl_trigger_task: Error in config loading logging: {e}")

    config_env_path = os.getenv("ENRICHMENT_CONFIG_PATH", "/app/config/enrichment_policy.yml")
    candidate_paths = [
        Path(config_env_path),
        Path(__file__).resolve().parent / "config" / "enrichment_policy.yml",
        Path(__file__).resolve().parent.parent / "config" / "enrichment_policy.yml",
    ]

    for path in candidate_paths:
        if not path:
            continue
        if not path.is_absolute():
            path = (Path(__file__).resolve().parent / path).resolve()
        if not path.exists():
            continue
        try:
            with path.open("r", encoding="utf-8") as cfg:
                config = yaml.safe_load(cfg) or {}
                loaded_tags = config.get("crawl4ai", {}).get("trigger_tags")
                if isinstance(loaded_tags, list) and loaded_tags:
                    trigger_tags = [str(tag).strip() for tag in loaded_tags if str(tag).strip()]
                    trigger_tags = list(dict.fromkeys(trigger_tags))
                    try:
                        logger.info(
                            f"Crawl trigger tags loaded from config: {str(path)}, tags_count={len(trigger_tags)}"
                        )
                    except Exception as log_err:
                        logger.error(f"Error in logger.info: {log_err}")
                else:
                    try:
                        logger.debug(
                            f"Crawl trigger tags list empty in config, using defaults: {str(path)}"
                        )
                    except Exception as log_err:
                        logger.error(f"Error in logger.debug: {log_err}")
            break
        except Exception as config_error:
            try:
                logger.warning(
                    f"Failed to load crawl trigger tags: {str(path)}, error={str(config_error)}"
                )
            except Exception as log_err:
                logger.error(f"Error in logger.warning: {log_err}, original_error={str(config_error)}")
    else:
        try:
            logger.debug(
                f"Using default crawl trigger tags: tags_count={len(trigger_tags)}"
            )
        except Exception as log_err:
            logger.error(f"Error in logger.debug (default tags): {log_err}")
    
    try:
        logger.info("create_crawl_trigger_task: Creating CrawlTriggerTask instance")
    except Exception as e:
        logger.error(f"create_crawl_trigger_task: Error before creating task: {e}")
    
    try:
        task = CrawlTriggerTask(
            redis_url=redis_url,
            trigger_tags=trigger_tags,
            db_dsn=database_url
        )
        logger.info("create_crawl_trigger_task: CrawlTriggerTask created, calling start()")
    except Exception as e:
        logger.error(f"create_crawl_trigger_task: Error creating CrawlTriggerTask: {e}", exc_info=True)
        raise
    
    try:
        await task.start()
    except Exception as e:
        logger.error(f"create_crawl_trigger_task: Error in task.start(): {e}", exc_info=True)
        raise

async def create_post_persistence_task():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ PostPersistenceWorker."""
    redis_url = os.getenv("REDIS_URL", "redis://redis:6379")
    database_url = os.getenv("DATABASE_URL", "postgresql+asyncpg://postgres:postgres@supabase-db:5432/postgres")
    worker = PostPersistenceWorker(redis_url=redis_url, database_url=database_url)
    await worker.initialize()
    await worker.start()

async def create_retagging_task():
    """Context7: –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ Retagging Task."""
    try:
        redis_url = os.getenv("REDIS_URL", "redis://redis:6379")
        logger.info("Creating RetaggingTask instance")
        task = RetaggingTask(redis_url)
        logger.info("RetaggingTask instance created, calling start()")
        await task.start()
        # Context7: –ù–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –∏–∑ start() - –∑–∞–¥–∞—á–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –≤ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–º —Ü–∏–∫–ª–µ
    except Exception as e:
        logger.warning(
            "RetaggingTask skipped",
            error=str(e),
            error_type=type(e).__name__,
            error_repr=repr(e),
            exc_info=True
        )
        raise

async def create_album_assembler_task():
    """Context7: –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ Album Assembler Task (Phase 2-4)."""
    try:
        import redis.asyncio as redis
        from sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker
        from event_bus import EventPublisher, RedisStreamsClient
        from api.services.s3_storage import S3StorageService
        from run_all_tasks_vision_helper import get_s3_config_from_env
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Redis
        redis_url = os.getenv("REDIS_URL", "redis://redis:6379")
        # Context7: AlbumAssemblerTask –∏—Å–ø–æ–ª—å–∑—É–µ—Ç redis.asyncio.Redis –Ω–∞–ø—Ä—è–º—É—é
        redis_client = redis.Redis.from_url(redis_url, decode_responses=False)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î
        db_url = os.getenv("DATABASE_URL", "postgresql+asyncpg://postgres:postgres@supabase-db:5432/postgres")
        if db_url.startswith("postgresql://") or db_url.startswith("postgres://"):
            db_url = db_url.replace("postgresql://", "postgresql+asyncpg://", 1).replace("postgres://", "postgresql+asyncpg://", 1)
        
        engine = create_async_engine(db_url)
        async_session = async_sessionmaker(engine, expire_on_commit=False)
        db_session = async_session()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è EventPublisher
        # Context7: EventPublisher —Ç—Ä–µ–±—É–µ—Ç RedisStreamsClient (–æ–±—ë—Ä—Ç–∫—É –Ω–∞–¥ redis.Redis)
        redis_streams_client = RedisStreamsClient(redis_url)
        await redis_streams_client.connect()
        event_publisher = EventPublisher(redis_streams_client)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è S3 (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è vision summary)
        s3_service = None
        try:
            s3_config = get_s3_config_from_env()
            if s3_config and s3_config.get('access_key_id') and s3_config.get('secret_access_key'):
                s3_service = S3StorageService(
                    endpoint_url=s3_config['endpoint_url'],
                    access_key_id=s3_config['access_key_id'],
                    secret_access_key=s3_config['secret_access_key'],
                    bucket_name=s3_config['bucket_name'],
                    region=s3_config.get('region', 'ru-central-1')
                )
                logger.info("S3 service initialized for album assembler")
        except Exception as e:
            logger.warning(f"S3 service not available for album assembler: {e}")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
        task = AlbumAssemblerTask(
            redis_client=redis_client,
            db_session=db_session,
            event_publisher=event_publisher,
            s3_service=s3_service
        )
        
        logger.info("AlbumAssemblerTask created and starting...")
        await task.start()
        # Context7: –ù–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –∏–∑ start() - –∑–∞–¥–∞—á–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–º —Ü–∏–∫–ª–µ
        
    except Exception as e:
        logger.error(f"Failed to create AlbumAssemblerTask: {e}", exc_info=True)
        raise


async def create_trend_worker_task():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ TrendDetectionWorker (reactive —Ç—Ä–µ–Ω–¥—ã)."""
    redis_url = os.getenv("REDIS_URL", "redis://redis:6379")
    database_url = os.getenv(
        "DATABASE_URL",
        "postgresql+asyncpg://postgres:postgres@supabase-db:5432/postgres",
    )
    qdrant_url = os.getenv("QDRANT_URL", "http://qdrant:6333")
    worker = TrendDetectionWorker(
        redis_url=redis_url,
        database_url=database_url,
        qdrant_url=qdrant_url,
    )
    await worker.start()


async def create_trend_editor_agent_task():
    """Context7: –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ TrendEditorAgent (—Ä–µ–¥–∞–∫—Ç–æ—Ä –∫–∞—Ä—Ç–æ—á–µ–∫ —Ç—Ä–µ–Ω–¥–æ–≤)."""
    agent = await create_trend_editor_agent()
    await agent.start()

async def create_vision_analysis_task():
    """Context7: –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ Vision Analysis Task."""
    try:
        # Context7: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ sys.path –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ api –º–æ–¥—É–ª—é (cross-service import)
        # –í production worker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ api –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¥–æ—Å—Ç—É–ø–µ–Ω —á–µ—Ä–µ–∑ volume mount –∏–ª–∏ –≤ –æ–±—Ä–∞–∑–µ
        import sys
        import os
        
        # –í–∞—Ä–∏–∞–Ω—Ç 1: /opt/telegram-assistant/api (volume mount)
        api_mount = '/opt/telegram-assistant/api'
        if os.path.exists(api_mount) and api_mount not in sys.path:
            sys.path.insert(0, api_mount)
            logger.debug(f"Added {api_mount} to sys.path for api imports")
        
        # –í–∞—Ä–∏–∞–Ω—Ç 2: project_root (dev)
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
        api_dev = os.path.join(project_root, 'api')
        if os.path.exists(api_dev) and api_dev not in sys.path:
            sys.path.insert(0, api_dev)
            logger.debug(f"Added {api_dev} to sys.path for api imports")
        
        from tasks.vision_analysis_task import create_vision_analysis_task as create_task
        
        redis_url = os.getenv("REDIS_URL", "redis://redis:6379")
        # Context7: VisionAnalysisTask —Ç—Ä–µ–±—É–µ—Ç asyncpg –¥—Ä–∞–π–≤–µ—Ä –¥–ª—è SQLAlchemy async
        database_url = os.getenv("DATABASE_URL", "postgresql+asyncpg://postgres:postgres@supabase-db:5432/postgres")
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è asyncpg, –∞ –Ω–µ psycopg2
        if database_url.startswith("postgresql://"):
            database_url = database_url.replace("postgresql://", "postgresql+asyncpg://", 1)
            logger.debug(f"Converted database_url to use asyncpg: {database_url.split('@')[0]}@...")
        
        # Context7: –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ env
        s3_config = get_s3_config_from_env()
        vision_config = get_vision_config_from_env()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
        task = await create_task(
            redis_url=redis_url,
            database_url=database_url,
            s3_config=s3_config,
            vision_config=vision_config
        )
        
        logger.info("VisionAnalysisTask created successfully")
        await task.start()
        # Context7: –ù–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –∏–∑ start() - –∑–∞–¥–∞—á–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –≤ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–º —Ü–∏–∫–ª–µ
        
    except ValueError as e:
        # Context7: –ï—Å–ª–∏ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç credentials - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–¥–∞—á—É —Å warning
        logger.warning(f"VisionAnalysisTask skipped: {e}")
        logger.warning("–î–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è Vision Analysis —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ GIGACHAT_CLIENT_ID –∏ GIGACHAT_CLIENT_SECRET")
    except Exception as e:
        logger.error(f"Failed to create VisionAnalysisTask: {e}", exc_info=True)
        raise

async def main():
    """–ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö tasks —Å supervisor."""
    print("üöÄ Starting worker with supervisor...")
    
    # [C7-ID: METRICS-REGISTRATION] –ò–º–ø–æ—Ä—Ç –º–µ—Ç—Ä–∏–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏
    # –≠—Ç–∏ –∏–º–ø–æ—Ä—Ç—ã –Ω—É–∂–Ω—ã –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –º–µ—Ç—Ä–∏–∫–∏ –±—ã–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã —á–µ—Ä–µ–∑ /metrics endpoint
    import metrics
    from metrics import posts_processed_total  # –î–ª—è Grafana dashboard
    from event_bus import posts_in_queue_total, stream_pending_size  # –ú–µ—Ç—Ä–∏–∫–∏ –æ—á–µ—Ä–µ–¥–µ–π
    from ai_providers.gigachain_adapter import tagging_requests_total, tagging_latency_seconds
    from ai_providers.embedding_service import embedding_requests_total, embedding_latency_seconds
    from tasks.tagging_task import tagging_processed_total
    from tasks.indexing_task import indexing_processed_total
    # Context7: –ò–º–ø–æ—Ä—Ç –º–µ—Ç—Ä–∏–∫ S3 –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏
    try:
        from shared.s3_storage.service import (
            s3_operations_total,
            s3_upload_duration_seconds,
            s3_file_size_bytes,
            s3_compression_ratio
        )
        logger.debug("S3 metrics imported successfully")
    except ImportError:
        logger.warning("S3 metrics not available (shared.s3_storage may not be loaded)")
    # Context7: –ò–º–ø–æ—Ä—Ç –º–µ—Ç—Ä–∏–∫ —Ä–µ—Ç–µ–≥–≥–∏–Ω–≥–∞ –¥–ª—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏
    try:
        from tasks.retagging_task import (
            retagging_processed_total,
            retagging_duration_seconds,
            retagging_dlq_total,
            retagging_skipped_total
        )
    except ImportError:
        logger.debug("RetaggingTask metrics not available (module may not be loaded)")
    
    # Context7: –ò–º–ø–æ—Ä—Ç –º–µ—Ç—Ä–∏–∫ album assembler –¥–ª—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ (Phase 4)
    try:
        from tasks.album_assembler_task import (
            albums_parsed_total,
            albums_assembled_total,
            album_assembly_lag_seconds,
            album_items_count_gauge,
            album_vision_summary_size_bytes,
            album_aggregation_duration_ms
        )
    except ImportError:
        logger.debug("AlbumAssemblerTask metrics not available (module may not be loaded)")
    
    # Context7: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ –Ω—É–ª–µ–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ Prometheus
    # –ú–µ—Ç—Ä–∏–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã —Ö–æ—Ç—è –±—ã —Ä–∞–∑, —á—Ç–æ–±—ã Prometheus –∏—Ö —É–≤–∏–¥–µ–ª
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è posts_processed_total –¥–ª—è –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π stage/success
        for stage in ['parsing', 'tagging', 'enrichment', 'indexing']:
            for success in ['true', 'false', 'error', 'skip', 'attempt']:
                posts_processed_total.labels(stage=stage, success=success).inc(0)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è posts_in_queue_total –¥–ª—è –æ—Å–Ω–æ–≤–Ω—ã—Ö —Å—Ç—Ä–∏–º–æ–≤
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∏–º–µ–Ω–∞ —Å—Ç—Ä–∏–º–æ–≤ (–∫–ª—é—á–∏ —Å–ª–æ–≤–∞—Ä—è STREAMS)
        from event_bus import STREAMS
        if STREAMS:
            for stream_name in STREAMS.keys():
                posts_in_queue_total.labels(queue=stream_name, status='total').set(0)
                posts_in_queue_total.labels(queue=stream_name, status='pending').set(0)
                posts_in_queue_total.labels(queue=stream_name, status='new').set(0)
                stream_pending_size.labels(stream=stream_name).set(0)
        else:
            # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∏–º–µ–Ω–∞ —Å—Ç—Ä–∏–º–æ–≤
            for stream_name in ['posts.parsed', 'posts.tagged', 'posts.enriched', 'posts.indexed']:
                posts_in_queue_total.labels(queue=stream_name, status='total').set(0)
                posts_in_queue_total.labels(queue=stream_name, status='pending').set(0)
                posts_in_queue_total.labels(queue=stream_name, status='new').set(0)
                stream_pending_size.labels(stream=stream_name).set(0)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ digest worker
        for stage in ["generate", "send"]:
            for status in ["success", "failed"]:
                digest_jobs_processed_total.labels(stage=stage, status=status).inc(0)
        for status in ["success", "failed"]:
            digest_worker_generation_seconds.labels(status=status).observe(0)
            digest_worker_send_seconds.labels(status=status).observe(0)

        logger.info("Metrics initialized with zero values")
    except Exception as e:
        logger.warning(f"Failed to initialize metrics: {e}", error=str(e))
    
    # –ó–∞–ø—É—Å–∫ HTTP —Å–µ—Ä–≤–µ—Ä–∞ –¥–ª—è –º–µ—Ç—Ä–∏–∫
    from prometheus_client import start_http_server
    metrics_port = int(os.getenv("METRICS_PORT", "8001"))
    print(f"Starting metrics server on port {metrics_port}", flush=True)
    try:
        start_http_server(metrics_port)
        print(f"Metrics server started on port {metrics_port}", flush=True)
        logger.info(f"Metrics server started on port {metrics_port}")
    except OSError as e:
        if e.errno == 98:  # Address already in use
            print(f"Metrics server already running on port {metrics_port}", flush=True)
            logger.warning(f"Metrics server already running on port {metrics_port}")
        else:
            print(f"Error starting metrics server: {e}", flush=True)
            raise
    
    supervisor = TaskSupervisor()
    
    # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è tasks
    supervisor.register_task(TaskConfig(
        name="tagging",
        task_func=create_tagging_task,
        max_retries=5,
        initial_backoff=1.0,
        max_backoff=60.0,
        backoff_multiplier=2.0
    ))
    
    supervisor.register_task(TaskConfig(
        name="enrichment",
        task_func=create_enrichment_task,
        max_retries=5,
        initial_backoff=1.0,
        max_backoff=60.0,
        backoff_multiplier=2.0
    ))
    
    supervisor.register_task(TaskConfig(
        name="indexing",
        task_func=create_indexing_task,
        max_retries=5,
        initial_backoff=1.0,
        max_backoff=60.0,
        backoff_multiplier=2.0
    ))
    
    supervisor.register_task(TaskConfig(
        name="tag_persistence",
        task_func=create_tag_persistence_task,
        max_retries=5,
        initial_backoff=1.0,
        max_backoff=60.0,
        backoff_multiplier=2.0
    ))
    
    supervisor.register_task(TaskConfig(
        name="crawl_trigger",
        task_func=create_crawl_trigger_task,
        max_retries=5,
        initial_backoff=1.0,
        max_backoff=60.0,
        backoff_multiplier=2.0
    ))

    # Post persistence –¥–æ–ª–∂–µ–Ω –∏–¥—Ç–∏ –ø–µ—Ä–≤—ã–º —ç—Ç–∞–ø–æ–º –ø–æ—Å–ª–µ parsed
    supervisor.register_task(TaskConfig(
        name="post_persistence",
        task_func=create_post_persistence_task,
        max_retries=5,
        initial_backoff=1.0,
        max_backoff=60.0,
        backoff_multiplier=2.0
    ))
    
    # Context7: Vision Analysis Task (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, —Ç—Ä–µ–±—É–µ—Ç—Å—è GigaChat credentials)
    supervisor.register_task(TaskConfig(
        name="vision_analysis",
        task_func=create_vision_analysis_task,
        max_retries=5,
        initial_backoff=1.0,
        max_backoff=60.0,
        backoff_multiplier=2.0
    ))
    
    # Context7: Retagging Task (–ø–æ–¥–ø–∏—Å–∞–Ω –Ω–∞ posts.vision.analyzed)
    supervisor.register_task(TaskConfig(
        name="retagging",
        task_func=create_retagging_task,
        max_retries=5,
        initial_backoff=1.0,
        max_backoff=60.0,
        backoff_multiplier=2.0
    ))
    
    # Context7: Album Assembler Task (Phase 2-4)
    supervisor.register_task(TaskConfig(
        name="album_assembler",
        task_func=create_album_assembler_task,
        max_retries=5,
        initial_backoff=1.0,
        max_backoff=60.0,
        backoff_multiplier=2.0
    ))

    supervisor.register_task(TaskConfig(
        name="trend_detection",
        task_func=create_trend_worker_task,
        max_retries=5,
        initial_backoff=1.0,
        max_backoff=60.0,
        backoff_multiplier=2.0
    ))
    
    # Context7: Trend Editor Agent –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∫–∞—Ä—Ç–æ—á–µ–∫
    supervisor.register_task(TaskConfig(
        name="trend_editor",
        task_func=create_trend_editor_agent_task,
        max_retries=5,
        initial_backoff=1.0,
        max_backoff=60.0,
        backoff_multiplier=2.0
    ))
    
    supervisor.register_task(TaskConfig(
        name="digest_worker",
        task_func=create_digest_worker_task,
        max_retries=5,
        initial_backoff=1.0,
        max_backoff=60.0,
        backoff_multiplier=2.0
    ))
    supervisor.register_task(TaskConfig(
        name="digest_context_observer",
        task_func=create_digest_context_task,
        max_retries=5,
        initial_backoff=1.0,
        max_backoff=60.0,
        backoff_multiplier=2.0
    ))
    
    try:
        await supervisor.start_all()
    except KeyboardInterrupt:
        print("üõë Stopping supervisor...")
        await supervisor.stop_all()
    except Exception as e:
        print(f"‚ùå Supervisor error: {e}")
        await supervisor.stop_all()
        raise

if __name__ == "__main__":
    asyncio.run(main())
