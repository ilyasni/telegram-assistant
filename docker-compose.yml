services:
  # PostgreSQL Database
  supabase-db:
    image: supabase/postgres:15.1.1.78
    environment:
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_MAX_CONNECTIONS: 200
    volumes:
      - supabase_data:/var/lib/postgresql/data
      - ./supabase/volumes/db/init:/docker-entrypoint-initdb.d
    networks:
      telegram-network:
        aliases:
          - db
    profiles:
      - core

  # Kong API Gateway
  kong:
    image: kong:3.4
    environment:
      KONG_DATABASE: "off"
      KONG_DECLARATIVE_CONFIG: /var/lib/kong/kong.yml
      KONG_DNS_ORDER: LAST,A,CNAME
      KONG_PLUGINS: cors,request-termination
      KONG_DNS_VALID_TTL: 1
      KONG_DNS_STALE_TTL: 0
      KONG_DNS_NOT_FOUND_TTL: 0
      KONG_DNS_ERROR_TTL: 1
      KONG_NGINX_PROXY_PROXY_BUFFER_SIZE: 160k
      KONG_NGINX_PROXY_PROXY_BUFFERS: 64 160k
    volumes:
      - ./supabase/volumes/api/kong.yml:/var/lib/kong/kong.yml:ro
    networks:
      - telegram-network
    profiles:
      - core

  # PostgREST API
  rest:
    image: postgrest/postgrest:v12.0.2
    environment:
      PGRST_DB_URI: postgresql://postgres:${POSTGRES_PASSWORD}@supabase-db:5432/${POSTGRES_DB}
      PGRST_DB_SCHEMAS: public,storage,graphql_public,telegram_bot
      PGRST_DB_ANON_ROLE: anon
      PGRST_JWT_SECRET: ${JWT_SECRET}
      PGRST_DB_USE_LEGACY_GUCS: "false"
    depends_on:
      - supabase-db
    networks:
      - telegram-network
    profiles:
      - core

  # Postgres Meta (критичен для Studio)
  meta:
    image: supabase/postgres-meta:v0.80.0
    environment:
      PG_META_PORT: 8080
      PG_META_DB_URL: postgresql://postgres:postgres@supabase-db:5432/postgres
      PG_META_DB_HOST__db: supabase-db
      PG_META_DB_PORT__db: "5432"
      PG_META_DB_NAME__db: postgres
      PG_META_DB_USER__db: postgres
      PG_META_DB_PASSWORD__db: postgres
    depends_on:
      - supabase-db
    networks:
      - telegram-network
    profiles:
      - core

  # Supabase Studio
  supabase-studio:
    image: supabase/studio:latest
    environment:
      # Basic configuration
      SUPABASE_URL: http://kong:8000
      SUPABASE_API_URL: http://kong:8000
      # Explicit Studio URLs to avoid localhost fallbacks
      STUDIO_SUPABASE_URL: http://kong:8000
      STUDIO_PG_META_URL: http://meta:8080
      KONG_URL: http://kong:8000
      # Disable lints in self-hosted
      STUDIO_DISABLE_LINTS: "true"
      DISABLE_LINTS: "true"
      POSTGRES_META_URL: http://meta:8080
      # Direct connection to meta
      POSTGRES_META_HOST: meta
      POSTGRES_META_PORT: 8080
      # (no direct DB connections; Studio uses Kong and pg-meta)
      # API Keys
      SUPABASE_ANON_KEY: ${ANON_KEY}
      SUPABASE_SERVICE_KEY: ${SERVICE_KEY}
      # Public URLs
      NEXT_PUBLIC_SUPABASE_URL: https://${SUPABASE_HOST}
      NEXT_PUBLIC_SUPABASE_ANON_KEY: ${ANON_KEY}
      SUPABASE_STUDIO_URL: https://${SUPABASE_HOST}
      NEXT_PUBLIC_SUPABASE_STUDIO_URL: https://${SUPABASE_HOST}
      # Project settings
      SUPABASE_PROJECT_REF: default
      SUPABASE_PROJECT_ID: default
      # JWT Secret
      SUPABASE_JWT_SECRET: ${JWT_SECRET}
      # API URLs
      SUPABASE_REST_URL: http://kong:8000/rest/v1
      SUPABASE_AUTH_URL: http://kong:8000/auth/v1
      SUPABASE_REALTIME_URL: http://kong:8000/realtime/v1
      SUPABASE_STORAGE_URL: http://kong:8000/storage/v1
      SUPABASE_FUNCTIONS_URL: http://kong:8000/functions/v1
      SUPABASE_GRAPHQL_URL: http://kong:8000/graphql/v1
      # Rename displayed project name in Studio (keep ref "default")
      STUDIO_PROJECTS: '[{"ref":"default","name":"Telegram Assistant"}]'
    depends_on:
      - supabase-db
      - rest
      - auth
      - meta
    networks:
      - telegram-network
    profiles:
      - core

  # Realtime
  realtime:
    image: supabase/realtime:v2.25.50
    environment:
      DB_HOST: supabase-db
      DB_PASSWORD: ${POSTGRES_PASSWORD}
      DB_NAME: ${POSTGRES_DB}
      DB_USER: postgres
      JWT_SECRET: ${JWT_SECRET}
      API_JWT_SECRET: ${JWT_SECRET}
      PORT: 4000
    depends_on:
      - supabase-db
    networks:
      - telegram-network
    profiles:
      - core

  # Storage
  storage:
    image: supabase/storage-api:v0.46.4
    environment:
      POSTGREST_URL: http://rest:3000
      DATABASE_URL: postgresql://postgres:${POSTGRES_PASSWORD}@supabase-db:5432/${POSTGRES_DB}
      FILE_SIZE_LIMIT: 52428800
      STORAGE_BACKEND: file
      GLOBAL_S3_BUCKET: telegram-assistant-storage
      ANON_KEY: ${ANON_KEY}
      SERVICE_KEY: ${SERVICE_KEY}
      JWT_SECRET: ${JWT_SECRET}
    depends_on:
      - supabase-db
      - rest
    networks:
      - telegram-network
    profiles:
      - core


  # Redis Cache
  redis:
    image: redis:7-alpine
    volumes:
      - redis_data:/data
    networks:
      - telegram-network
    # Context7 best practice: health check для Redis
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    profiles:
      - core

  # Grafana Dashboard
  grafana:
    image: grafana/grafana:latest
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GF_SECURITY_ADMIN_PASSWORD}
      GF_SERVER_ROOT_URL: https://grafana.${DOMAIN}/
      GF_SERVER_DOMAIN: grafana.${DOMAIN}
      GF_SERVER_ENFORCE_DOMAIN: "true"
      GF_SERVER_SERVE_FROM_SUB_PATH: "false"
      GF_SECURITY_COOKIE_SECURE: "true"
      GF_SECURITY_COOKIE_SAMESITE: "strict"
      GF_SECURITY_STRICT_TRANSPORT_SECURITY: "true"
      GF_SECURITY_STRICT_TRANSPORT_SECURITY_MAX_AGE_SECONDS: 31536000
      GF_SECURITY_STRICT_TRANSPORT_SECURITY_PRELOAD: "true"
      GF_SECURITY_STRICT_TRANSPORT_SECURITY_SUBDOMAINS: "true"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/dashboards:ro
      - ./grafana/provisioning:/etc/grafana/provisioning
    networks:
      - telegram-network
    profiles:
      - analytics

  # Prometheus Metrics
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./prometheus/alerts.yml:/etc/prometheus/alerts.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    ports:
      - "9090:9090"
    networks:
      - telegram-network
    profiles:
      - monitoring

  # Caddy Reverse Proxy
  caddy:
    image: caddy:2-alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile
      - ./webapp:/var/www/webapp:ro
      - caddy_data:/data
      - caddy_config:/config
    networks:
      - telegram-network
    profiles:
      - core

  # Qdrant Vector DB
  qdrant:
    image: qdrant/qdrant:latest
    volumes:
      - qdrant_data:/qdrant/storage
    networks:
      - telegram-network
    # qdrant образ не содержит curl/wget — оставляем без healthcheck
    profiles:
      - core

  # Neo4j Graph Database
  neo4j:
    image: neo4j:5.15-community
    environment:
      NEO4J_AUTH: ${NEO4J_USER:-neo4j}/${NEO4J_PASSWORD:-changeme}
      NEO4J_PLUGINS: '["apoc"]'
      NEO4J_server_memory_heap_initial__size: 512m
      NEO4J_server_memory_heap_max__size: 2G
      NEO4J_server_memory_pagecache_size: 512m
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
    networks:
      - telegram-network
    healthcheck:
      # Явно указываем протокол neo4j:// и адрес для избежания SSL ошибок
      test: ["CMD", "cypher-shell", "-a", "neo4j://neo4j:7687", "-u", "neo4j", "-p", "${NEO4J_PASSWORD:-changeme}", "RETURN 1;"]
      interval: 20s
      timeout: 5s
      retries: 15
      start_period: 40s
    profiles:
      - core

  # GigaChat Proxy (gpt2giga) - локальная сборка с официальным gpt2giga
  gpt2giga-proxy:
    build:
      context: ./gpt2giga-proxy
      dockerfile: Dockerfile
    ports:
      - "8090:8090"
    environment:
      # Базовые настройки (из документации)
      PROXY_HOST: "0.0.0.0"
      PROXY_PORT: "8090"
      
      # GigaChat API (обязательные)
      GIGACHAT_CREDENTIALS: ${GIGACHAT_CREDENTIALS}
      GIGACHAT_SCOPE: ${GIGACHAT_SCOPE:-GIGACHAT_API_PERS}
      GIGACHAT_BASE_URL: "https://gigachat.devices.sberbank.ru/api/v1"
      GIGACHAT_MODEL: "GigaChat"
      
      # Таймауты и retry (Context7 best practice)
      GPT2GIGA_TIMEOUT: "600"
      
      # Verbose logging для диагностики
      GPT2GIGA_VERBOSE: "True"
      
      # Передача модели (из документации)
      GPT2GIGA_PASS_MODEL: "True"
      
      # SSL сертификаты (отключаем для dev/test)
      GIGACHAT_VERIFY_SSL_CERTS: "False"
      
      # Embeddings модель
      GPT2GIGA_EMBEDDINGS: "EmbeddingsGigaR"
    networks:
      - telegram-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8090/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 5
    profiles:
      - core

  # Neo4j Health Server
  neo4j-health:
    build: ./neo4j
    environment:
      NEO4J_URI: ${NEO4J_URI:-neo4j://neo4j:7687}
      NEO4J_USER: ${NEO4J_USER:-neo4j}
      NEO4J_PASSWORD: ${NEO4J_PASSWORD}
    depends_on:
      neo4j:
        condition: service_healthy
    networks:
      - telegram-network
    ports:
      - "7475:7475"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7475/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - core

  # API Service (FastAPI)
  api:
    build: ./api
    environment:
      DATABASE_URL: postgresql://postgres:${POSTGRES_PASSWORD}@supabase-db:5432/${POSTGRES_DB}
      REDIS_URL: redis://redis:6379
      QDRANT_URL: http://qdrant:6333
      JWT_SECRET: ${JWT_SECRET}
      TELEGRAM_BOT_TOKEN: ${TELEGRAM_BOT_TOKEN}
      BOT_WEBHOOK_SECRET: ${BOT_WEBHOOK_SECRET}
      BOT_PUBLIC_URL: ${BOT_PUBLIC_URL}
      DEFAULT_TENANT_ID: ${DEFAULT_TENANT_ID}
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      ENVIRONMENT: ${ENVIRONMENT:-production}
      # Context7 best practice: timezone для корректной работы с временем
      TZ: Europe/Moscow
    volumes:
      - ./webapp:/app/webapp:ro
      # Context7 best practice: монтируем timezone данные хоста
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
    depends_on:
      - supabase-db
      - redis
      - qdrant
    networks:
      - telegram-network
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import urllib.request as u; import sys;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntry:\n    u.urlopen(\"http://localhost:8000/health\"); sys.exit(0)\nexcept Exception:\n    try:\n        u.urlopen(\"http://localhost:8000/api/health\"); sys.exit(0)\n    except Exception:\n        sys.exit(1)\n'"]
      interval: 10s
      timeout: 5s
      retries: 10
    profiles:
      - core

  # Worker Service
  worker:
    build: ./worker
    environment:
      DATABASE_URL: postgresql://postgres:${POSTGRES_PASSWORD}@supabase-db:5432/${POSTGRES_DB}
      REDIS_URL: redis://redis:6379
      QDRANT_URL: http://qdrant:6333
      NEO4J_URL: ${NEO4J_URL:-bolt://neo4j:7687}
      NEO4J_URI: ${NEO4J_URI:-neo4j://neo4j:7687}
      NEO4J_USER: ${NEO4J_USER:-neo4j}
      NEO4J_PASSWORD: ${NEO4J_PASSWORD:-changeme}
      # AI Provider Configuration with Fallback
      # Primary: GigaChat через gpt2giga-proxy
      OPENAI_API_BASE: ${OPENAI_API_BASE:-http://gpt2giga-proxy:8090/v1}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-dummy}
      # Флаг для использования прокси GigaChat
      USE_GIGACHAT_PROXY: ${USE_GIGACHAT_PROXY:-true}
      GIGACHAT_PROXY_URL: ${GIGACHAT_PROXY_URL:-http://gpt2giga-proxy:8090}
      # Убираем GIGACHAT_ACCESS_TOKEN - используем только прокси
      # GIGACHAT_ACCESS_TOKEN: ${GIGACHAT_ACCESS_TOKEN}
      # Fallback: OpenRouter
      OPENROUTER_API_KEY: ${OPENROUTER_API_KEY}
      # DEV Feature Flags
      FEATURE_ALLOW_NON_UUID_IDS: ${FEATURE_ALLOW_NON_UUID_IDS:-true}
      OPENROUTER_MODEL: ${OPENROUTER_MODEL:-qwen/qwen-2.5-72b-instruct:free}
      OPENROUTER_API_BASE: ${OPENROUTER_API_BASE:-https://openrouter.ai/api/v1}
      ENRICHMENT_CONFIG_PATH: /app/config/enrichment_policy.yml
      # Feature flags
      FEATURE_NEO4J_ENABLED: ${FEATURE_NEO4J_ENABLED:-true}
      FEATURE_GIGACHAT_ENABLED: ${FEATURE_GIGACHAT_ENABLED:-true}
      FEATURE_OPENROUTER_ENABLED: ${FEATURE_OPENROUTER_ENABLED:-true}
      FEATURE_CRAWL4AI_ENABLED: ${FEATURE_CRAWL4AI_ENABLED:-true}
      LEGACY_REDIS_CONSUMER_ENABLED: ${LEGACY_REDIS_CONSUMER_ENABLED:-false}
      # Enrichment limits bypass
      ENRICHMENT_SKIP_LIMITS: ${ENRICHMENT_SKIP_LIMITS:-true}
      ENRICHMENT_LIMITS_SAMPLING: ${ENRICHMENT_LIMITS_SAMPLING:-0}
      # Indexing fallback
      INDEXER_EMBED_IF_MISSING: ${INDEXER_EMBED_IF_MISSING:-true}
      # Надёжность вызовов провайдеров
      AI_HTTP_TIMEOUT_SEC: ${AI_HTTP_TIMEOUT_SEC:-30}
      AI_MAX_RETRIES: ${AI_MAX_RETRIES:-3}
      AI_RETRY_BACKOFF_SEC: ${AI_RETRY_BACKOFF_SEC:-5}
      LOG_LEVEL: ${LOG_LEVEL:-DEBUG}
      ENVIRONMENT: ${ENVIRONMENT:-production}
      EMBEDDING_MODEL: ${EMBEDDING_MODEL:-hash-based}
      EMBEDDING_DIMENSION: ${EMBEDDING_DIMENSION:-128}
      # Dispatch smoke to validate routing before handler
      DISPATCH_SMOKE: ${DISPATCH_SMOKE:-false}
      # Force temporary handler for debugging
      INDEXER_FORCE_TMP: ${INDEXER_FORCE_TMP:-false}
      # Unbuffered stdout for immediate print visibility
      PYTHONUNBUFFERED: 1
      # Context7 best practice: timezone для корректной работы с временем
      TZ: Europe/Moscow
    volumes:
      - ./worker/config:/app/config:ro
      # Context7 best practice: монтируем timezone данные хоста
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
    depends_on:
      supabase-db:
        condition: service_started
      redis:
        condition: service_healthy
      qdrant:
        condition: service_started
      neo4j:
        condition: service_healthy
    networks:
      - telegram-network
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import urllib.request as u; import sys;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntry:\n    u.urlopen(\"http://localhost:8000/health\"); sys.exit(0)\nexcept Exception:\n    sys.exit(0)\n'"]
      interval: 10s
      timeout: 5s
      retries: 10
    profiles:
      - core

  # Telethon Ingest Service
  telethon-ingest:
    build: ./telethon-ingest
    environment:
      MASTER_API_ID: ${MASTER_API_ID}
      MASTER_API_HASH: ${MASTER_API_HASH}
      ENCRYPTION_KEY: ${ENCRYPTION_KEY}
      DATABASE_URL: postgresql://postgres:${POSTGRES_PASSWORD}@supabase-db:5432/${POSTGRES_DB}?connect_timeout=5&application_name=telethon-ingest&keepalives=1&keepalives_idle=30&keepalives_interval=10&keepalives_count=3
      REDIS_URL: redis://redis:6379
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      ENVIRONMENT: ${ENVIRONMENT:-production}
      INGEST_HEALTH_PORT: 8011
      # Context7: Incremental parsing configuration
      FEATURE_INCREMENTAL_PARSING_ENABLED: ${FEATURE_INCREMENTAL_PARSING_ENABLED:-true}
      PARSER_MODE_OVERRIDE: ${PARSER_MODE_OVERRIDE:-auto}
      PARSER_SCHEDULER_INTERVAL_SEC: ${PARSER_SCHEDULER_INTERVAL_SEC:-300}
      PARSER_HISTORICAL_HOURS: ${PARSER_HISTORICAL_HOURS:-24}
      PARSER_INCREMENTAL_MINUTES: ${PARSER_INCREMENTAL_MINUTES:-5}
      PARSER_LPA_MAX_AGE_HOURS: ${PARSER_LPA_MAX_AGE_HOURS:-48}
      PARSER_MAX_CONCURRENCY: ${PARSER_MAX_CONCURRENCY:-4}
      PARSER_RETRY_MAX: ${PARSER_RETRY_MAX:-3}
      # Context7 best practice: отладка и логи
      PYTHONUNBUFFERED: 1
      PYTHONASYNCIODEBUG: 1
      # Context7 best practice: timezone для корректной работы с временем
      TZ: Europe/Moscow
      # Отключаем прокси на время диагностики
      HTTP_PROXY: ""
      HTTPS_PROXY: ""
      ALL_PROXY: ""
    volumes:
      - ./telethon-ingest/sessions:/app/sessions  # персистентность сессий
      # Context7 best practice: монтируем timezone данные хоста
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
    depends_on:
      supabase-db:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - telegram-network
    restart: unless-stopped  # автоматический рестарт
    # Context7 best practice: health check для telethon-ingest на порту 8011
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8011/health', timeout=5)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    profiles:
      - core

  # Crawl4AI Enrichment Service
  crawl4ai:
    build:
      context: ./crawl4ai
      dockerfile: Dockerfile
    environment:
      REDIS_URL: redis://redis:6379
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_DB: 0
      DATABASE_URL: postgresql://postgres:${POSTGRES_PASSWORD}@supabase-db:5432/${POSTGRES_DB}
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      ENRICHMENT_CONFIG_PATH: /app/config/enrichment_policy.yml
      FEATURE_CRAWL4AI_ENABLED: ${FEATURE_CRAWL4AI_ENABLED:-true}
      MAX_CONCURRENT_CRAWLS: 3
      TMPDIR: /tmp/playwright
      # Crawl4AI specific settings
      CRAWL_USER_AGENT: "TelegramAssistant/1.0 (Crawl4AI Bot)"
      CRAWL_RESPECT_ROBOTS: "true"
      CRAWL_MAX_DEPTH: "3"
      CRAWL_TIMEOUT_SEC: "30"
      CRAWL_CACHE_TTL_SEC: "3600"
      CRAWL_PER_DOMAIN_CONCURRENCY: "2"
      # Consumer group
      CRAWL_STREAM: stream:posts:crawl
      CRAWL_GROUP: crawl4ai_workers
    volumes:
      - ./worker/config:/app/config:ro
      - /tmp/playwright:/tmp/playwright
    shm_size: "1gb"
    ulimits:
      nofile: 65535
    depends_on:
      - redis
      - supabase-db
    networks:
      - telegram-network
    # [C7-ID: DOCKER-CRAWL4AI-002] - Ресурсные ограничения для Playwright
    mem_limit: 2G
    cpus: 1.0
    healthcheck:
      test: ["CMD", "sh", "-c", "printf 'GET / HTTP/1.1\r\nHost: localhost\r\nConnection: close\r\n\r\n' | nc -w 1 localhost 8080 >/dev/null 2>&1 || echo 'OK'"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s
    profiles:
      - core

volumes:
  supabase_data:
  redis_data:
  grafana_data:
  prometheus_data:
  caddy_data:
  caddy_config:
  qdrant_data:
  neo4j_data:
  neo4j_logs:

networks:
  telegram-network:
    driver: bridge

