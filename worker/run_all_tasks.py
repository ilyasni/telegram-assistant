#!/usr/bin/env python3
"""
–ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö worker tasks —Å supervisor pattern –¥–ª—è –∞–≤—Ç–æ–ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞.
"""

import asyncio
import sys
import os
import logging

# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s'
)
logger = logging.getLogger(__name__)

from supervisor import TaskSupervisor, TaskConfig
from tasks.tagging_task import TaggingTask
from tasks.enrichment_task import EnrichmentWorker
from tasks.indexing_task import IndexingTask
from tasks.tag_persistence_task import TagPersistenceTask
from tasks.crawl_trigger_task import CrawlTriggerTask
from tasks.post_persistence_task import PostPersistenceWorker
from run_all_tasks_vision_helper import get_s3_config_from_env, get_vision_config_from_env

async def create_tagging_task():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ tagging task."""
    redis_url = os.getenv("REDIS_URL", "redis://redis:6379")
    # –ü–æ best practices TaggingTask –Ω–µ —Ç—Ä–µ–±—É–µ—Ç DATABASE_URL –∏ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Ç–æ–ª—å–∫–æ redis_url
    task = TaggingTask(redis_url)
    await task.start()
    # Context7: –ù–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –∏–∑ start() - –∑–∞–¥–∞—á–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –≤ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–º —Ü–∏–∫–ª–µ

async def create_enrichment_task():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ enrichment task."""
    # Enrichment task —Ç—Ä–µ–±—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
    from tasks.enrichment_task import main as enrichment_main
    await enrichment_main()

async def create_indexing_task():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ indexing task."""
    redis_url = os.getenv("REDIS_URL", "redis://redis:6379")
    qdrant_url = os.getenv("QDRANT_URL", "http://localhost:6333")
    neo4j_url = os.getenv("NEO4J_URL", "bolt://localhost:7687")
    
    task = IndexingTask(redis_url, qdrant_url, neo4j_url)
    await task.start()

async def create_tag_persistence_task():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ tag persistence task."""
    redis_url = os.getenv("REDIS_URL", "redis://redis:6379")
    database_url = os.getenv("DATABASE_URL", "postgresql://postgres:postgres@localhost:5432/postgres")
    
    task = TagPersistenceTask(redis_url, database_url)
    await task.start()

async def create_crawl_trigger_task():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ crawl trigger task."""
    redis_url = os.getenv("REDIS_URL", "redis://redis:6379")
    
    # –¢—Ä–∏–≥–≥–µ—Ä–Ω—ã–µ —Ç–µ–≥–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    trigger_tags = [
        'longread', 'research', 'paper', 'release', 'law',
        'deepdive', 'analysis', 'report', 'study', 'whitepaper'
    ]
    
    task = CrawlTriggerTask(
        redis_url=redis_url,
        trigger_tags=trigger_tags
    )
    await task.start()

async def create_post_persistence_task():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ PostPersistenceWorker."""
    redis_url = os.getenv("REDIS_URL", "redis://redis:6379")
    database_url = os.getenv("DATABASE_URL", "postgresql+asyncpg://postgres:postgres@supabase-db:5432/postgres")
    worker = PostPersistenceWorker(redis_url=redis_url, database_url=database_url)
    await worker.initialize()
    await worker.start()

async def create_vision_analysis_task():
    """Context7: –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ Vision Analysis Task."""
    try:
        # Context7: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ sys.path –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ api –º–æ–¥—É–ª—é (cross-service import)
        # –í production worker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ api –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¥–æ—Å—Ç—É–ø–µ–Ω —á–µ—Ä–µ–∑ volume mount –∏–ª–∏ –≤ –æ–±—Ä–∞–∑–µ
        import sys
        import os
        
        # –í–∞—Ä–∏–∞–Ω—Ç 1: /opt/telegram-assistant/api (volume mount)
        api_mount = '/opt/telegram-assistant/api'
        if os.path.exists(api_mount) and api_mount not in sys.path:
            sys.path.insert(0, api_mount)
            logger.debug(f"Added {api_mount} to sys.path for api imports")
        
        # –í–∞—Ä–∏–∞–Ω—Ç 2: project_root (dev)
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
        api_dev = os.path.join(project_root, 'api')
        if os.path.exists(api_dev) and api_dev not in sys.path:
            sys.path.insert(0, api_dev)
            logger.debug(f"Added {api_dev} to sys.path for api imports")
        
        from tasks.vision_analysis_task import create_vision_analysis_task as create_task
        
        redis_url = os.getenv("REDIS_URL", "redis://redis:6379")
        # Context7: VisionAnalysisTask —Ç—Ä–µ–±—É–µ—Ç asyncpg –¥—Ä–∞–π–≤–µ—Ä –¥–ª—è SQLAlchemy async
        database_url = os.getenv("DATABASE_URL", "postgresql+asyncpg://postgres:postgres@supabase-db:5432/postgres")
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è asyncpg, –∞ –Ω–µ psycopg2
        if database_url.startswith("postgresql://"):
            database_url = database_url.replace("postgresql://", "postgresql+asyncpg://", 1)
            logger.debug(f"Converted database_url to use asyncpg: {database_url.split('@')[0]}@...")
        
        # Context7: –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ env
        s3_config = get_s3_config_from_env()
        vision_config = get_vision_config_from_env()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
        task = await create_task(
            redis_url=redis_url,
            database_url=database_url,
            s3_config=s3_config,
            vision_config=vision_config
        )
        
        logger.info("VisionAnalysisTask created successfully")
        await task.start()
        # Context7: –ù–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –∏–∑ start() - –∑–∞–¥–∞—á–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –≤ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–º —Ü–∏–∫–ª–µ
        
    except ValueError as e:
        # Context7: –ï—Å–ª–∏ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç credentials - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–¥–∞—á—É —Å warning
        logger.warning(f"VisionAnalysisTask skipped: {e}")
        logger.warning("–î–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è Vision Analysis —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ GIGACHAT_CLIENT_ID –∏ GIGACHAT_CLIENT_SECRET")
    except Exception as e:
        logger.error(f"Failed to create VisionAnalysisTask: {e}", exc_info=True)
        raise

async def main():
    """–ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö tasks —Å supervisor."""
    print("üöÄ Starting worker with supervisor...")
    
    # [C7-ID: METRICS-REGISTRATION] –ò–º–ø–æ—Ä—Ç –º–µ—Ç—Ä–∏–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏
    # –≠—Ç–∏ –∏–º–ø–æ—Ä—Ç—ã –Ω—É–∂–Ω—ã –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –º–µ—Ç—Ä–∏–∫–∏ –±—ã–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã —á–µ—Ä–µ–∑ /metrics endpoint
    import metrics
    from metrics import posts_processed_total  # –î–ª—è Grafana dashboard
    from event_bus import posts_in_queue_total, stream_pending_size  # –ú–µ—Ç—Ä–∏–∫–∏ –æ—á–µ—Ä–µ–¥–µ–π
    from ai_providers.gigachain_adapter import tagging_requests_total, tagging_latency_seconds
    from ai_providers.embedding_service import embedding_requests_total, embedding_latency_seconds
    from tasks.tagging_task import tagging_processed_total
    from tasks.indexing_task import indexing_processed_total
    
    # Context7: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ –Ω—É–ª–µ–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ Prometheus
    # –ú–µ—Ç—Ä–∏–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã —Ö–æ—Ç—è –±—ã —Ä–∞–∑, —á—Ç–æ–±—ã Prometheus –∏—Ö —É–≤–∏–¥–µ–ª
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è posts_processed_total –¥–ª—è –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π stage/success
        for stage in ['parsing', 'tagging', 'enrichment', 'indexing']:
            for success in ['true', 'false', 'error', 'skip', 'attempt']:
                posts_processed_total.labels(stage=stage, success=success).inc(0)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è posts_in_queue_total –¥–ª—è –æ—Å–Ω–æ–≤–Ω—ã—Ö —Å—Ç—Ä–∏–º–æ–≤
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∏–º–µ–Ω–∞ —Å—Ç—Ä–∏–º–æ–≤ (–∫–ª—é—á–∏ —Å–ª–æ–≤–∞—Ä—è STREAMS)
        from event_bus import STREAMS
        if STREAMS:
            for stream_name in STREAMS.keys():
                posts_in_queue_total.labels(queue=stream_name, status='total').set(0)
                posts_in_queue_total.labels(queue=stream_name, status='pending').set(0)
                posts_in_queue_total.labels(queue=stream_name, status='new').set(0)
                stream_pending_size.labels(stream=stream_name).set(0)
        else:
            # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∏–º–µ–Ω–∞ —Å—Ç—Ä–∏–º–æ–≤
            for stream_name in ['posts.parsed', 'posts.tagged', 'posts.enriched', 'posts.indexed']:
                posts_in_queue_total.labels(queue=stream_name, status='total').set(0)
                posts_in_queue_total.labels(queue=stream_name, status='pending').set(0)
                posts_in_queue_total.labels(queue=stream_name, status='new').set(0)
                stream_pending_size.labels(stream=stream_name).set(0)
        
        logger.info("Metrics initialized with zero values")
    except Exception as e:
        logger.warning(f"Failed to initialize metrics: {e}", error=str(e))
    
    # –ó–∞–ø—É—Å–∫ HTTP —Å–µ—Ä–≤–µ—Ä–∞ –¥–ª—è –º–µ—Ç—Ä–∏–∫
    from prometheus_client import start_http_server
    metrics_port = int(os.getenv("METRICS_PORT", "8001"))
    print(f"Starting metrics server on port {metrics_port}", flush=True)
    try:
        start_http_server(metrics_port)
        print(f"Metrics server started on port {metrics_port}", flush=True)
        logger.info(f"Metrics server started on port {metrics_port}")
    except OSError as e:
        if e.errno == 98:  # Address already in use
            print(f"Metrics server already running on port {metrics_port}", flush=True)
            logger.warning(f"Metrics server already running on port {metrics_port}")
        else:
            print(f"Error starting metrics server: {e}", flush=True)
            raise
    
    supervisor = TaskSupervisor()
    
    # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è tasks
    supervisor.register_task(TaskConfig(
        name="tagging",
        task_func=create_tagging_task,
        max_retries=5,
        initial_backoff=1.0,
        max_backoff=60.0,
        backoff_multiplier=2.0
    ))
    
    supervisor.register_task(TaskConfig(
        name="enrichment",
        task_func=create_enrichment_task,
        max_retries=5,
        initial_backoff=1.0,
        max_backoff=60.0,
        backoff_multiplier=2.0
    ))
    
    supervisor.register_task(TaskConfig(
        name="indexing",
        task_func=create_indexing_task,
        max_retries=5,
        initial_backoff=1.0,
        max_backoff=60.0,
        backoff_multiplier=2.0
    ))
    
    supervisor.register_task(TaskConfig(
        name="tag_persistence",
        task_func=create_tag_persistence_task,
        max_retries=5,
        initial_backoff=1.0,
        max_backoff=60.0,
        backoff_multiplier=2.0
    ))
    
    supervisor.register_task(TaskConfig(
        name="crawl_trigger",
        task_func=create_crawl_trigger_task,
        max_retries=5,
        initial_backoff=1.0,
        max_backoff=60.0,
        backoff_multiplier=2.0
    ))

    # Post persistence –¥–æ–ª–∂–µ–Ω –∏–¥—Ç–∏ –ø–µ—Ä–≤—ã–º —ç—Ç–∞–ø–æ–º –ø–æ—Å–ª–µ parsed
    supervisor.register_task(TaskConfig(
        name="post_persistence",
        task_func=create_post_persistence_task,
        max_retries=5,
        initial_backoff=1.0,
        max_backoff=60.0,
        backoff_multiplier=2.0
    ))
    
    # Context7: Vision Analysis Task (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, —Ç—Ä–µ–±—É–µ—Ç—Å—è GigaChat credentials)
    supervisor.register_task(TaskConfig(
        name="vision_analysis",
        task_func=create_vision_analysis_task,
        max_retries=5,
        initial_backoff=1.0,
        max_backoff=60.0,
        backoff_multiplier=2.0
    ))
    
    try:
        await supervisor.start_all()
    except KeyboardInterrupt:
        print("üõë Stopping supervisor...")
        await supervisor.stop_all()
    except Exception as e:
        print(f"‚ùå Supervisor error: {e}")
        await supervisor.stop_all()
        raise

if __name__ == "__main__":
    asyncio.run(main())
